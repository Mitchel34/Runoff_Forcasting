{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a63397d",
   "metadata": {},
   "source": [
    "# Runoff Forecasting using LSTM and Transformer Models\n",
    "\n",
    "This notebook implements and evaluates LSTM and Transformer models for runoff forecasting at two different stations (21609641 and 20380357)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5755a",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Import libraries such as NumPy, Pandas, Matplotlib, TensorFlow, and potentially PyTorch for data handling, visualization, and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99618ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from joblib import load as joblib_load\n",
    "# import torch # Uncomment if using PyTorch\n",
    "\n",
    "# Evaluation metrics (if not using standard libraries)\n",
    "# from sklearn.metrics import mean_squared_error, r2_score # Example\n",
    "# Define custom metrics if needed (e.g., NSE, PBIAS, CC)\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "# print(\"PyTorch Version:\", torch.__version__) # Uncomment if using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03acd4",
   "metadata": {},
   "source": [
    "## Load and Explore Data\n",
    "Load the raw and processed data for both stations. Perform exploratory data analysis to understand patterns and anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths (adjust as necessary)\n",
    "PROCESSED_DATA_DIR = 'data/processed'\n",
    "MODELS_DIR = 'models'\n",
    "RESULTS_DIR = 'results'\n",
    "PLOTS_DIR = os.path.join(RESULTS_DIR, 'plots')\n",
    "METRICS_DIR = os.path.join(RESULTS_DIR, 'metrics')\n",
    "\n",
    "# Create results directories if they don't exist\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Load processed data info (example for one station to show structure)\n",
    "station_id_s1 = '21609641'\n",
    "station_id_s2 = '20380357'\n",
    "\n",
    "try:\n",
    "    test_data_s1 = np.load(os.path.join(PROCESSED_DATA_DIR, 'test', f'{station_id_s1}.npz'))\n",
    "    print(f\"Loaded test data keys for {station_id_s1}: {list(test_data_s1.keys())}\")\n",
    "    print(f\"  X_test shape: {test_data_s1['X_test'].shape}\")\n",
    "    print(f\"  y_test_scaled shape: {test_data_s1['y_test_scaled'].shape}\")\n",
    "    print(f\"  nwm_test_original shape: {test_data_s1['nwm_test_original'].shape}\")\n",
    "    print(f\"  usgs_test_original shape: {test_data_s1['usgs_test_original'].shape}\")\n",
    "    test_data_s1.close() # Close the file\n",
    "\n",
    "    test_data_s2 = np.load(os.path.join(PROCESSED_DATA_DIR, 'test', f'{station_id_s2}.npz'))\n",
    "    print(f\"\\nLoaded test data keys for {station_id_s2}: {list(test_data_s2.keys())}\")\n",
    "    print(f\"  X_test shape: {test_data_s2['X_test'].shape}\")\n",
    "    print(f\"  y_test_scaled shape: {test_data_s2['y_test_scaled'].shape}\")\n",
    "    print(f\"  nwm_test_original shape: {test_data_s2['nwm_test_original'].shape}\")\n",
    "    print(f\"  usgs_test_original shape: {test_data_s2['usgs_test_original'].shape}\")\n",
    "    test_data_s2.close()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading test data: {e}. Please ensure preprocessing was run.\")\n",
    "\n",
    "# --- (Optional) Exploratory Data Analysis on loaded arrays ---\n",
    "# Example: Plot time series for one lead time from the loaded arrays\n",
    "try:\n",
    "    test_data_s1 = np.load(os.path.join(PROCESSED_DATA_DIR, 'test', f'{station_id_s1}.npz'))\n",
    "    usgs_s1_lead1 = test_data_s1['usgs_test_original'][:, 0] # Lead time 1\n",
    "    nwm_s1_lead1 = test_data_s1['nwm_test_original'][:, 0] # Lead time 1\n",
    "    # Create dummy time index for plotting\n",
    "    time_index_s1 = np.arange(len(usgs_s1_lead1))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time_index_s1, usgs_s1_lead1, label='USGS Observation (S1, Lead 1)')\n",
    "    plt.plot(time_index_s1, nwm_s1_lead1, label='NWM Forecast (S1, Lead 1)', alpha=0.7)\n",
    "    plt.title(f'Station {station_id_s1} - Observed vs. NWM Forecast (Lead 1 - Test Set)')\n",
    "    plt.xlabel('Time Step Index')\n",
    "    plt.ylabel('Runoff (cms)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    test_data_s1.close()\n",
    "except FileNotFoundError:\n",
    "    print(\"Skipping EDA plot for Station 1: Test data not found.\")\n",
    "\n",
    "try:\n",
    "    test_data_s2 = np.load(os.path.join(PROCESSED_DATA_DIR, 'test', f'{station_id_s2}.npz'))\n",
    "    usgs_s2_lead1 = test_data_s2['usgs_test_original'][:, 0] # Lead time 1\n",
    "    nwm_s2_lead1 = test_data_s2['nwm_test_original'][:, 0] # Lead time 1\n",
    "    time_index_s2 = np.arange(len(usgs_s2_lead1))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time_index_s2, usgs_s2_lead1, label='USGS Observation (S2, Lead 1)')\n",
    "    plt.plot(time_index_s2, nwm_s2_lead1, label='NWM Forecast (S2, Lead 1)', alpha=0.7)\n",
    "    plt.title(f'Station {station_id_s2} - Observed vs. NWM Forecast (Lead 1 - Test Set)')\n",
    "    plt.xlabel('Time Step Index')\n",
    "    plt.ylabel('Runoff (cms)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    test_data_s2.close()\n",
    "except FileNotFoundError:\n",
    "    print(\"Skipping EDA plot for Station 2: Test data not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f94057",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "Implement data preprocessing steps, including cleaning, aligning NWM forecasts with USGS observations, creating input-output sequences, and splitting data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing was done in preprocess.py ---\n",
    "# This cell is kept for context but the actual data loading for evaluation\n",
    "# will happen in the evaluation step using the .npz files.\n",
    "print(\"Data preprocessing is assumed to be completed by 'src/preprocess.py'.\")\n",
    "print(\"Loading preprocessed data from .npz files in the evaluation step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b348a",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "Evaluate the trained models on the test set using metrics such as CC (Correlation Coefficient), RMSE (Root Mean Squared Error), PBIAS (Percent Bias), and NSE (Nash-Sutcliffe Efficiency). Compare results against raw NWM forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Metrics Functions ---\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    # Ensure inputs are numpy arrays\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Filter out NaN values to avoid computation errors\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan # Return NaN if no valid pairs\n",
    "    return np.sqrt(np.mean((y_true[mask] - y_pred[mask])**2))\n",
    "\n",
    "def calculate_cc(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if np.sum(mask) < 2: # Need at least 2 points for correlation\n",
    "        return np.nan\n",
    "    # Check for zero standard deviation\n",
    "    if np.std(y_true[mask]) == 0 or np.std(y_pred[mask]) == 0:\n",
    "        return np.nan\n",
    "    return np.corrcoef(y_true[mask], y_pred[mask])[0, 1]\n",
    "\n",
    "def calculate_pbias(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    sum_true = np.sum(y_true[mask])\n",
    "    if sum_true == 0:\n",
    "        return np.nan # Avoid division by zero\n",
    "    return 100 * np.sum(y_pred[mask] - y_true[mask]) / sum_true\n",
    "\n",
    "def calculate_nse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    mean_true = np.mean(y_true[mask])\n",
    "    numerator = np.sum((y_true[mask] - y_pred[mask])**2)\n",
    "    denominator = np.sum((y_true[mask] - mean_true)**2)\n",
    "    if denominator == 0:\n",
    "        # Handle case where observed data is constant: NSE is -inf if pred != true, 1 if pred == true\n",
    "        return 1.0 if numerator == 0 else -np.inf\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "# --- Helper Function to Load Data and Scaler ---\n",
    "def load_evaluation_data(station_id):\n",
    "    test_data_path = os.path.join(PROCESSED_DATA_DIR, 'test', f'{station_id}.npz')\n",
    "    scaler_path = os.path.join(PROCESSED_DATA_DIR, 'scalers', f'{station_id}_y_scaler.joblib')\n",
    "    if not os.path.exists(test_data_path) or not os.path.exists(scaler_path):\n",
    "        print(f\"Warning: Test data or scaler not found for station {station_id}. Skipping evaluation.\")\n",
    "        return None, None, None, None, None\n",
    "    try:\n",
    "        data = np.load(test_data_path)\n",
    "        X_test = data['X_test']\n",
    "        y_test_scaled = data['y_test_scaled'] # Scaled errors\n",
    "        nwm_test_original = data['nwm_test_original']\n",
    "        usgs_test_original = data['usgs_test_original']\n",
    "        data.close()\n",
    "        y_scaler = joblib_load(scaler_path)\n",
    "        print(f\"Loaded test data and y_scaler for station {station_id}.\")\n",
    "        # Ensure y_test_scaled has the correct shape (samples, lead_times)\n",
    "        if len(y_test_scaled.shape) == 1:\n",
    "             # This might happen if preprocess saved it flattened, reshape based on nwm/usgs data\n",
    "             n_samples = X_test.shape[0]\n",
    "             n_lead_times = nwm_test_original.shape[1]\n",
    "             if len(y_test_scaled) == n_samples * n_lead_times:\n",
    "                 y_test_scaled = y_test_scaled.reshape(n_samples, n_lead_times)\n",
    "                 print(f\"Reshaped y_test_scaled to {y_test_scaled.shape}\")\n",
    "             else:\n",
    "                 print(f\"Warning: Cannot reshape y_test_scaled for station {station_id}. Unexpected length.\")\n",
    "                 return None, None, None, None, None\n",
    "        elif len(y_test_scaled.shape) != 2:\n",
    "             print(f\"Warning: y_test_scaled for station {station_id} has unexpected shape {y_test_scaled.shape}.\")\n",
    "             return None, None, None, None, None\n",
    "\n",
    "        return X_test, y_test_scaled, nwm_test_original, usgs_test_original, y_scaler\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for station {station_id}: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# --- Helper Function to Evaluate a Model ---\n",
    "def run_evaluation(station_id, model_type):\n",
    "    model_filename = f\"{station_id}_{model_type.lower()}_best.keras\"\n",
    "    model_path = os.path.join(MODELS_DIR, model_filename)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found: {model_path}. Skipping evaluation.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    X_test, y_test_scaled, nwm_test_original, usgs_test_original, y_scaler = load_evaluation_data(station_id)\n",
    "    if X_test is None:\n",
    "        return None, None, None, None\n",
    "\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"Loaded model {model_filename}\")\n",
    "\n",
    "        # Predict scaled errors\n",
    "        predicted_errors_scaled = model.predict(X_test)\n",
    "        print(f\"Predicted errors (scaled) shape: {predicted_errors_scaled.shape}\")\n",
    "\n",
    "        # Ensure prediction shape matches y_test_scaled shape\n",
    "        if predicted_errors_scaled.shape != y_test_scaled.shape:\n",
    "             print(f\"Warning: Shape mismatch between predicted ({predicted_errors_scaled.shape}) and true scaled errors ({y_test_scaled.shape}). Trying to reshape prediction.\")\n",
    "             # Attempt common reshape if prediction is missing lead time dim\n",
    "             if len(predicted_errors_scaled.shape) == 1 and len(y_test_scaled.shape) == 2:\n",
    "                 if predicted_errors_scaled.shape[0] == y_test_scaled.shape[0] * y_test_scaled.shape[1]:\n",
    "                     predicted_errors_scaled = predicted_errors_scaled.reshape(y_test_scaled.shape)\n",
    "                     print(f\"Reshaped prediction to {predicted_errors_scaled.shape}\")\n",
    "                 else:\n",
    "                     print(\"Cannot reshape prediction due to element count mismatch.\")\n",
    "                     return None, None, None, None\n",
    "             # Attempt common reshape if prediction has extra dim\n",
    "             elif len(predicted_errors_scaled.shape) == 3 and predicted_errors_scaled.shape[-1] == 1 and len(y_test_scaled.shape) == 2:\n",
    "                 if predicted_errors_scaled.shape[0] == y_test_scaled.shape[0] and predicted_errors_scaled.shape[1] == y_test_scaled.shape[1]:\n",
    "                      predicted_errors_scaled = predicted_errors_scaled.squeeze(-1)\n",
    "                      print(f\"Reshaped prediction to {predicted_errors_scaled.shape}\")\n",
    "                 else:\n",
    "                     print(\"Cannot reshape prediction due to dimension mismatch.\")\n",
    "                     return None, None, None, None\n",
    "             else:\n",
    "                 print(\"Unhandled shape mismatch.\")\n",
    "                 return None, None, None, None\n",
    "\n",
    "        # Inverse transform predicted errors\n",
    "        # Scaler expects (n_samples * n_features, 1) or (n_samples, n_features)\n",
    "        n_samples = predicted_errors_scaled.shape[0]\n",
    "        n_lead_times = predicted_errors_scaled.shape[1]\n",
    "        predicted_errors_unscaled = y_scaler.inverse_transform(predicted_errors_scaled.reshape(-1, n_lead_times))\n",
    "        # predicted_errors_unscaled = predicted_errors_unscaled.reshape(n_samples, n_lead_times) # Already in this shape\n",
    "        print(f\"Predicted errors (unscaled) shape: {predicted_errors_unscaled.shape}\")\n",
    "\n",
    "        # Calculate corrected NWM forecasts\n",
    "        corrected_nwm_forecasts = nwm_test_original - predicted_errors_unscaled\n",
    "        print(f\"Corrected NWM forecasts shape: {corrected_nwm_forecasts.shape}\")\n",
    "\n",
    "        # Calculate Metrics for each Lead Time\n",
    "        print(\"Calculating evaluation metrics per lead time...\")\n",
    "        metrics = {'lead_time': list(range(1, n_lead_times + 1))}\n",
    "        metric_funcs = {'CC': calculate_cc, 'RMSE': calculate_rmse, 'PBIAS': calculate_pbias, 'NSE': calculate_nse}\n",
    "\n",
    "        for metric_name, func in metric_funcs.items():\n",
    "            metrics[f'NWM_{metric_name}'] = []\n",
    "            metrics[f'Corrected_{metric_name}'] = []\n",
    "            for i in range(n_lead_times):\n",
    "                obs = usgs_test_original[:, i]\n",
    "                nwm_pred = nwm_test_original[:, i]\n",
    "                corrected_pred = corrected_nwm_forecasts[:, i]\n",
    "\n",
    "                # Handle potential NaNs from calculations or data\n",
    "                nwm_metric = func(obs, nwm_pred)\n",
    "                corrected_metric = func(obs, corrected_pred)\n",
    "\n",
    "                metrics[f'NWM_{metric_name}'].append(nwm_metric)\n",
    "                metrics[f'Corrected_{metric_name}'].append(corrected_metric)\n",
    "\n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        metrics_filename = os.path.join(METRICS_DIR, f\"{station_id}_{model_type.lower()}_evaluation_metrics.csv\")\n",
    "        metrics_df.to_csv(metrics_filename, index=False)\n",
    "        print(f\"Saved evaluation metrics to {metrics_filename}\")\n",
    "        print(metrics_df.head())\n",
    "\n",
    "        return metrics_df, usgs_test_original, nwm_test_original, corrected_nwm_forecasts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation for station {station_id} ({model_type}): {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None\n",
    "\n",
    "# --- Run Evaluation for Both Stations ---\n",
    "print(\"\\n--- Evaluating Station 21609641 (LSTM) ---\")\n",
    "metrics_s1, usgs_s1, nwm_s1, corrected_s1 = run_evaluation(station_id_s1, 'lstm')\n",
    "\n",
    "print(\"\\n--- Evaluating Station 20380357 (Transformer) ---\")\n",
    "metrics_s2, usgs_s2, nwm_s2, corrected_s2 = run_evaluation(station_id_s2, 'transformer')\n",
    "\n",
    "print(\"\\n--- Evaluation Summary --- \")\n",
    "if metrics_s1 is not None:\n",
    "    print(f\"\\nMetrics for Station {station_id_s1} (LSTM):\")\n",
    "    print(metrics_s1.describe())\n",
    "if metrics_s2 is not None:\n",
    "    print(f\"\\nMetrics for Station {station_id_s2} (Transformer):\")\n",
    "    print(metrics_s2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e6180",
   "metadata": {},
   "source": [
    "## Generate Visualizations\n",
    "Create box plots for observed, NWM, and corrected runoff for each lead time. Generate box plots for evaluation metrics across lead times (if multiple lead times were predicted/evaluated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Functions ---\n",
    "def plot_runoff_comparison(station_id, model_type, usgs_data, nwm_data, corrected_data):\n",
    "    if usgs_data is None or nwm_data is None or corrected_data is None:\n",
    "        print(f\"Skipping runoff comparison plot for {station_id}: Missing data.\")\n",
    "        return\n",
    "    n_lead_times = usgs_data.shape[1]\n",
    "    lead_times = list(range(1, n_lead_times + 1))\n",
    "\n",
    "    plt.figure(figsize=(18, 8)) # Wider figure\n",
    "    plot_data = []\n",
    "    for i in range(n_lead_times):\n",
    "        plot_data.append(pd.DataFrame({\n",
    "            'Runoff': usgs_data[:, i],\n",
    "            'Lead Time': lead_times[i],\n",
    "            'Type': 'Observed (USGS)'\n",
    "        }))\n",
    "        plot_data.append(pd.DataFrame({\n",
    "            'Runoff': nwm_data[:, i],\n",
    "            'Lead Time': lead_times[i],\n",
    "            'Type': 'NWM Forecast'\n",
    "        }))\n",
    "        plot_data.append(pd.DataFrame({\n",
    "            'Runoff': corrected_data[:, i],\n",
    "            'Lead Time': lead_times[i],\n",
    "            'Type': f'Corrected ({model_type.upper()})'\n",
    "        }))\n",
    "    plot_df = pd.concat(plot_data)\n",
    "\n",
    "    # Filter out potential NaNs before plotting\n",
    "    plot_df.dropna(subset=['Runoff'], inplace=True)\n",
    "\n",
    "    sns.boxplot(data=plot_df, x='Lead Time', y='Runoff', hue='Type', showfliers=False) # Hide outliers for clarity\n",
    "    plt.title(f'Runoff Comparison by Lead Time - Station {station_id}')\n",
    "    plt.xlabel('Lead Time (Hours)')\n",
    "    plt.ylabel('Runoff (cms)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Forecast Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "    plot_filename = os.path.join(PLOTS_DIR, f\"{station_id}_{model_type.lower()}_runoff_boxplot.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"Saved runoff comparison plot to {plot_filename}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics_comparison(station_id, model_type, metrics_df):\n",
    "    if metrics_df is None:\n",
    "        print(f\"Skipping metrics comparison plot for {station_id}: Missing metrics data.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    metric_plot_names = ['CC', 'RMSE', 'PBIAS', 'NSE']\n",
    "\n",
    "    for i, metric_name in enumerate(metric_plot_names):\n",
    "        # Check if metric columns exist\n",
    "        nwm_col = f'NWM_{metric_name}'\n",
    "        corrected_col = f'Corrected_{metric_name}'\n",
    "        if nwm_col not in metrics_df.columns or corrected_col not in metrics_df.columns:\n",
    "             print(f\"Warning: Skipping plot for metric '{metric_name}' - columns not found in DataFrame.\")\n",
    "             continue\n",
    "\n",
    "        # Melt dataframe for seaborn boxplot\n",
    "        melted_df = pd.melt(metrics_df,\n",
    "                            id_vars=['lead_time'],\n",
    "                            value_vars=[nwm_col, corrected_col],\n",
    "                            var_name='Forecast Type',\n",
    "                            value_name=metric_name)\n",
    "        # Clean up the 'Forecast Type' names\n",
    "        melted_df['Forecast Type'] = melted_df['Forecast Type'].str.replace(f'_{metric_name}', '').replace('Corrected', f'Corrected ({model_type.upper()})')\n",
    "\n",
    "        # Filter out potential NaNs before plotting\n",
    "        melted_df.dropna(subset=[metric_name], inplace=True)\n",
    "\n",
    "        sns.boxplot(data=melted_df, x='lead_time', y=metric_name, hue='Forecast Type', ax=axes[i], showfliers=False)\n",
    "        axes[i].set_title(f'{metric_name} Comparison')\n",
    "        axes[i].set_xlabel('Lead Time (Hours)')\n",
    "        axes[i].set_ylabel(metric_name)\n",
    "        # axes[i].legend(title='Forecast Type')\n",
    "        axes[i].legend().set_visible(False) # Hide individual legends, use main figure legend\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        axes[i].grid(axis='y', linestyle='--')\n",
    "\n",
    "    # Add a single legend to the figure\n",
    "    handles, labels = axes[0].get_legend_handles_labels() # Get handles/labels from one subplot\n",
    "    fig.legend(handles, labels, title='Forecast Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.suptitle(f'Evaluation Metrics Comparison by Lead Time - Station {station_id} ({model_type.upper()})', y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout\n",
    "    plot_filename = os.path.join(PLOTS_DIR, f\"{station_id}_{model_type.lower()}_metrics_boxplot.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"Saved metrics comparison plot to {plot_filename}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# --- Generate Plots ---\n",
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "plot_runoff_comparison(station_id_s1, 'lstm', usgs_s1, nwm_s1, corrected_s1)\n",
    "plot_metrics_comparison(station_id_s1, 'lstm', metrics_s1)\n",
    "\n",
    "plot_runoff_comparison(station_id_s2, 'transformer', usgs_s2, nwm_s2, corrected_s2)\n",
    "plot_metrics_comparison(station_id_s2, 'transformer', metrics_s2)\n",
    "\n",
    "# --- (Optional) Time Series Plot for a specific lead time ---\n",
    "def plot_timeseries_lead_time(station_id, model_type, usgs_data, nwm_data, corrected_data, lead_time_index=0):\n",
    "    if usgs_data is None or nwm_data is None or corrected_data is None:\n",
    "        print(f\"Skipping time series plot for {station_id}: Missing data.\")\n",
    "        return\n",
    "    if lead_time_index >= usgs_data.shape[1]:\n",
    "        print(f\"Skipping time series plot for {station_id}: Invalid lead_time_index {lead_time_index}.\")\n",
    "        return\n",
    "\n",
    "    lead_time_hour = lead_time_index + 1\n",
    "    time_index = np.arange(len(usgs_data[:, lead_time_index]))\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(time_index, usgs_data[:, lead_time_index], label=f'Observed (USGS)', color='blue')\n",
    "    plt.plot(time_index, nwm_data[:, lead_time_index], label=f'NWM Forecast', color='green', alpha=0.7, linestyle='--')\n",
    "    plt.plot(time_index, corrected_data[:, lead_time_index], label=f'Corrected ({model_type.upper()})', color='red', alpha=0.8)\n",
    "    plt.title(f'Station {station_id}: Forecast vs Actual Runoff (Lead Time {lead_time_hour} hr - Test Set)')\n",
    "    plt.xlabel('Time Step Index')\n",
    "    plt.ylabel('Runoff (cms)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n--- Generating Time Series Plots (Lead Time 1) ---\")\n",
    "plot_timeseries_lead_time(station_id_s1, 'lstm', usgs_s1, nwm_s1, corrected_s1, lead_time_index=0)\n",
    "plot_timeseries_lead_time(station_id_s2, 'transformer', usgs_s2, nwm_s2, corrected_s2, lead_time_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2ed97",
   "metadata": {},
   "source": [
    "## Compare Model Performance\n",
    "Analyze and compare the performance of LSTM and Transformer models for both stations. Highlight differences in behavior and accuracy based on the evaluation metrics and visualizations.\n",
    "\n",
    "**Station 21609641 (LSTM):**\n",
    "*   Analyze the LSTM model's performance based on RMSE, CC, PBIAS, NSE compared to the raw NWM forecast.\n",
    "*   Discuss the shape of the error distribution (from box plot). Is there bias? How wide is the spread?\n",
    "*   Examine the time series plot: Does the LSTM capture peaks and troughs better than NWM? Are there lags?\n",
    "\n",
    "**Station 20380357 (Transformer):**\n",
    "*   Analyze the Transformer model's performance similarly.\n",
    "*   Compare its metrics to the raw NWM forecast for this station.\n",
    "*   Discuss its error distribution and time series behavior.\n",
    "\n",
    "**Overall Comparison:**\n",
    "*   Which model type performed better overall, considering the metrics? (Note: They are applied to different stations here, so direct comparison is tricky unless the stations/data are very similar).\n",
    "*   Did one model type show specific strengths (e.g., capturing extremes, lower bias)?\n",
    "*   Relate performance differences to potential characteristics of the data for each station or the inherent differences between LSTM (sequential processing) and Transformer (attention mechanism) architectures.\n",
    "*   Discuss potential reasons for observed performance (e.g., data quality, sequence length choice, model complexity).\n",
    "*   Suggest future improvements or experiments (e.g., hyperparameter tuning, different features, longer sequences, different model variants)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
