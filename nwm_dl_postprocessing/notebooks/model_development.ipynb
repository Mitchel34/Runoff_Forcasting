{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5527b3da",
   "metadata": {},
   "source": [
    "# Model Development: Seq2Seq LSTM for Runoff Forecast Error Correction\n",
    "\n",
    "This notebook demonstrates the development and tuning of a Sequence-to-Sequence LSTM model for NWM runoff forecast error correction. We'll explore different model architectures and hyperparameters to find the best configuration.\n",
    "\n",
    "## Objectives\n",
    "1. Import and prepare the preprocessed data\n",
    "2. Implement the Seq2Seq LSTM architecture\n",
    "3. Perform hyperparameter tuning with temporal cross-validation\n",
    "4. Evaluate candidate models and select the best one\n",
    "5. Save the final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for importing project modules\n",
    "sys.path.append('..')\n",
    "from src.preprocess import DataPreprocessor\n",
    "from src.model import Seq2SeqLSTMModel\n",
    "from src.tuner import Seq2SeqTuner\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c32e55",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "First, let's load the preprocessed data for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "raw_data_path = \"../data/raw\"\n",
    "processed_data_path = \"../data/processed\"\n",
    "models_path = \"../models\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Initialize data preprocessor\n",
    "preprocessor = DataPreprocessor(\n",
    "    raw_data_path=raw_data_path,\n",
    "    processed_data_path=processed_data_path,\n",
    "    sequence_length=24  # Default: 24 hours of past data\n",
    ")\n",
    "\n",
    "# Process data for both streams\n",
    "stream_ids = [\"20380357\", \"21609641\"]\n",
    "data = preprocessor.process_data(stream_ids=stream_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training/validation data for the first stream\n",
    "stream_id = stream_ids[0]  # Using first stream for development\n",
    "X_encoder_train = data[stream_id]['train_val']['X_encoder']\n",
    "X_decoder_train = data[stream_id]['train_val']['X_decoder']\n",
    "y_train = data[stream_id]['train_val']['y']\n",
    "\n",
    "print(f\"Encoder input shape: {X_encoder_train.shape}\")\n",
    "print(f\"Decoder input shape: {X_decoder_train.shape}\")\n",
    "print(f\"Target output shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c5e61",
   "metadata": {},
   "source": [
    "## 2. Create Base Model\n",
    "\n",
    "Let's create a base Seq2Seq LSTM model to understand its architecture before tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a969936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base model\n",
    "base_model = Seq2SeqLSTMModel(\n",
    "    encoder_timesteps=X_encoder_train.shape[1],  # Number of timesteps in encoder sequence\n",
    "    encoder_features=X_encoder_train.shape[2],   # Number of features per timestep\n",
    "    decoder_timesteps=18,                        # 18 hours (lead times 1-18)\n",
    "    lstm_units=64,                               # Base number of LSTM units\n",
    "    dropout_rate=0.2,                            # Initial dropout rate\n",
    "    learning_rate=0.001,                         # Initial learning rate\n",
    "    num_layers=2                                 # Initial number of LSTM layers\n",
    ")\n",
    "\n",
    "# Build and display model architecture\n",
    "model = base_model.build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fae4e",
   "metadata": {},
   "source": [
    "## 3. Define TimeSeriesSplit for Temporal Cross-Validation\n",
    "\n",
    "For time series data, we need to ensure that our validation approach respects the temporal order of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TimeSeriesSplit for temporal cross-validation\n",
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Visualize the splits\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_encoder_train)):\n",
    "    plt.scatter(val_idx, [i] * len(val_idx), c='red', s=10, label='Validation' if i == 0 else '')\n",
    "    plt.scatter(train_idx, [i] * len(train_idx), c='blue', s=10, label='Training' if i == 0 else '')\n",
    "plt.title('TimeSeriesSplit Cross-validation')\n",
    "plt.ylabel('Split')\n",
    "plt.xlabel('Sample index')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7ce65",
   "metadata": {},
   "source": [
    "## 4. Basic Training without Hyperparameter Tuning\n",
    "\n",
    "Let's train a basic model on the first fold of our data to check if everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b80072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first split\n",
    "for train_idx, val_idx in tscv.split(X_encoder_train):\n",
    "    X_enc_fold_train, X_enc_fold_val = X_encoder_train[train_idx], X_encoder_train[val_idx]\n",
    "    X_dec_fold_train, X_dec_fold_val = X_decoder_train[train_idx], X_decoder_train[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "    break  # Only use first split for basic test\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "base_history = base_model.train(\n",
    "    X_enc_fold_train, X_dec_fold_train, y_fold_train,\n",
    "    validation_data=([X_enc_fold_val, X_dec_fold_val], y_fold_val),\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d55154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(base_history.history['loss'], label='Training Loss')\n",
    "plt.plot(base_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(base_history.history['mae'], label='Training MAE')\n",
    "plt.plot(base_history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Mean Absolute Error over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2eb6e",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with KerasTuner\n",
    "\n",
    "Now let's perform hyperparameter tuning using KerasTuner and TimeSeriesSplit validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tuner\n",
    "tuner = Seq2SeqTuner(\n",
    "    encoder_timesteps=X_encoder_train.shape[1],\n",
    "    encoder_features=X_encoder_train.shape[2],\n",
    "    decoder_timesteps=18,\n",
    "    project_name='nwm_seq2seq_tuning',\n",
    "    directory=models_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tuner\n",
    "tuner.setup_tuner(\n",
    "    tuner_type='hyperband',  # Using Hyperband algorithm for efficiency\n",
    "    max_trials=20,           # Reduced for notebook, use higher value for full search\n",
    "    executions_per_trial=1   # Number of times to train each trial\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter search with TimeSeriesSplit\n",
    "tuner_results, best_hps = tuner.search_with_time_series_cv(\n",
    "    X_encoder_train, \n",
    "    X_decoder_train, \n",
    "    y_train, \n",
    "    n_splits=3,        # Number of time series folds\n",
    "    batch_size=32,     # Batch size\n",
    "    epochs=30,         # Max epochs per trial\n",
    "    verbose=1          # Verbose output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf01eb",
   "metadata": {},
   "source": [
    "## 6. Examine Hyperparameter Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top performing trials\n",
    "top_trials = tuner_results.oracle.get_best_trials(5)\n",
    "print(\"Top 5 trials:\")\n",
    "for i, trial in enumerate(top_trials):\n",
    "    print(f\"\\nTrial {i+1} - Score: {trial.score:.5f}\")\n",
    "    print(f\"  LSTM Units: {trial.hyperparameters.values['lstm_units']}\")\n",
    "    print(f\"  Dropout Rate: {trial.hyperparameters.values['dropout_rate']}\")\n",
    "    print(f\"  Learning Rate: {trial.hyperparameters.values['learning_rate']}\")\n",
    "    print(f\"  Number of Layers: {trial.hyperparameters.values['num_layers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053a299",
   "metadata": {},
   "source": [
    "## 7. Build and Train Final Model with Best Hyperparameters\n",
    "\n",
    "Let's build and train the final model with the best hyperparameters on the full training+validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db38261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters\n",
    "best_hps = tuner_results.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"LSTM Units: {best_hps.get('lstm_units')}\")\n",
    "print(f\"Dropout Rate: {best_hps.get('dropout_rate')}\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Number of Layers: {best_hps.get('num_layers')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final model with best hyperparameters\n",
    "final_model = Seq2SeqLSTMModel(\n",
    "    encoder_timesteps=X_encoder_train.shape[1],\n",
    "    encoder_features=X_encoder_train.shape[2],\n",
    "    decoder_timesteps=18,\n",
    "    lstm_units=best_hps.get('lstm_units'),\n",
    "    dropout_rate=best_hps.get('dropout_rate'),\n",
    "    learning_rate=best_hps.get('learning_rate'),\n",
    "    num_layers=best_hps.get('num_layers')\n",
    ")\n",
    "\n",
    "final_model.build_model()\n",
    "final_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for final training\n",
    "final_callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(models_path, \"nwm_lstm_model_checkpoint.keras\"),\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad566ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training set\n",
    "final_history = final_model.train(\n",
    "    X_encoder_train, X_decoder_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,  # Higher value as we now use early stopping\n",
    "    callbacks=final_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(final_history.history['loss'], label='Training Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(final_history.history['mae'], label='Training MAE')\n",
    "plt.title('Mean Absolute Error over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe436e46",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6198cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model.save(os.path.join(models_path, \"nwm_lstm_model.keras\"))\n",
    "print(f\"Final model saved to {os.path.join(models_path, 'nwm_lstm_model.keras')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bf689",
   "metadata": {},
   "source": [
    "## 9. Additional Analysis: Impact of Hyperparameters\n",
    "\n",
    "Let's analyze how different hyperparameters affected model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots to visualize hyperparameter impact\n",
    "import kerastuner as kt\n",
    "\n",
    "# Extract trials data\n",
    "trials_df = pd.DataFrame([\n",
    "    {\n",
    "        'lstm_units': trial.hyperparameters.values['lstm_units'],\n",
    "        'dropout_rate': trial.hyperparameters.values['dropout_rate'],\n",
    "        'learning_rate': trial.hyperparameters.values['learning_rate'],\n",
    "        'num_layers': trial.hyperparameters.values['num_layers'],\n",
    "        'score': trial.score\n",
    "    } for trial in tuner_results.oracle.trials.values() if trial.score is not None\n",
    "])\n",
    "\n",
    "# Plot impact of each hyperparameter\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# LSTM Units\n",
    "sns.boxplot(x='lstm_units', y='score', data=trials_df, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Impact of LSTM Units on Performance')\n",
    "axes[0, 0].set_ylabel('Validation Loss')\n",
    "\n",
    "# Dropout Rate\n",
    "sns.boxplot(x='dropout_rate', y='score', data=trials_df, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Impact of Dropout Rate on Performance')\n",
    "axes[0, 1].set_ylabel('Validation Loss')\n",
    "\n",
    "# Learning Rate\n",
    "sns.boxplot(x='learning_rate', y='score', data=trials_df, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Impact of Learning Rate on Performance')\n",
    "axes[1, 0].set_ylabel('Validation Loss')\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Number of Layers\n",
    "sns.boxplot(x='num_layers', y='score', data=trials_df, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Impact of Number of Layers on Performance')\n",
    "axes[1, 1].set_ylabel('Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055854b",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Loaded and prepared the preprocessed data for model development\n",
    "2. Created a base Seq2Seq LSTM model and tested it on a validation fold\n",
    "3. Performed hyperparameter tuning using KerasTuner with TimeSeriesSplit validation\n",
    "4. Analyzed the impact of different hyperparameters on model performance\n",
    "5. Built and trained a final model with the best hyperparameters\n",
    "6. Saved the trained model for later use in forecast correction\n",
    "\n",
    "Next steps will include:\n",
    "- Evaluating the model on the held-out test set (Oct 2022 - Apr 2023)\n",
    "- Comparing model performance against the baseline persistence model\n",
    "- Visualizing forecast corrections and calculating evaluation metrics\n",
    "- Creating the required plots for the technical report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df736da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
