{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075cb914",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of NWM and USGS Time Series\n",
    "\n",
    "This notebook performs exploratory analysis of the National Water Model (NWM) forecasts and USGS observed runoff data.\n",
    "\n",
    "## Objectives:\n",
    "1. Understand the structure and properties of the data\n",
    "2. Identify patterns and trends in NWM forecasts and USGS observations\n",
    "3. Visualize forecast errors across different lead times\n",
    "4. Evaluate data quality issues such as missing values and outliers\n",
    "5. Identify potential features for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82fa1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a8f43",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Inspection\n",
    "\n",
    "First, let's load the USGS observed data and NWM forecast data for both streams.\n",
    "Based on the file inspection, we know that:\n",
    "\n",
    "- USGS files have columns: 'DateTime', 'USGSFlowValue', and quality code\n",
    "- NWM files have columns: 'model_initialization_time', 'model_output_valid_time', 'streamflow_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188502ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/CS 4440 AI/Runoff_Forcasting/Runoff_Forcasting/nwm_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'datetime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_id \u001b[38;5;129;01min\u001b[39;00m stream_ids:\n\u001b[0;32m---> 38\u001b[0m     usgs_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_usgs_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     nwm_df \u001b[38;5;241m=\u001b[39m load_nwm_data(stream_id)\n\u001b[1;32m     40\u001b[0m     data[stream_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musgs\u001b[39m\u001b[38;5;124m\"\u001b[39m: usgs_df,\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnwm\u001b[39m\u001b[38;5;124m\"\u001b[39m: nwm_df\n\u001b[1;32m     43\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mload_usgs_data\u001b[0;34m(stream_id)\u001b[0m\n\u001b[1;32m     11\u001b[0m usgs_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(usgs_files[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert to datetime\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m usgs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43musgs_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     14\u001b[0m usgs_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usgs_df\n",
      "File \u001b[0;32m~/Desktop/CS 4440 AI/Runoff_Forcasting/Runoff_Forcasting/nwm_env/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/CS 4440 AI/Runoff_Forcasting/Runoff_Forcasting/nwm_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'datetime'"
     ]
    }
   ],
   "source": [
    "# Define paths - adjust to match the actual data location\n",
    "data_path = \"../data/raw\"\n",
    "stream_ids = [\"20380357\", \"21609641\"]\n",
    "\n",
    "# Function to load USGS data with correct column names\n",
    "def load_usgs_data(stream_id):\n",
    "    usgs_files = glob(os.path.join(data_path, str(stream_id), \"*_Strt_*.csv\"))\n",
    "    if not usgs_files:\n",
    "        raise FileNotFoundError(f\"No USGS data files found for stream {stream_id}\")\n",
    "    \n",
    "    usgs_df = pd.read_csv(usgs_files[0])\n",
    "    # Convert to datetime with correct column name\n",
    "    usgs_df['DateTime'] = pd.to_datetime(usgs_df['DateTime'])\n",
    "    usgs_df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    return usgs_df\n",
    "\n",
    "# Function to load NWM data with correct column names\n",
    "def load_nwm_data(stream_id):\n",
    "    nwm_files = glob(os.path.join(data_path, str(stream_id), \"streamflow_*.csv\"))\n",
    "    if not nwm_files:\n",
    "        raise FileNotFoundError(f\"No NWM data files found for stream {stream_id}\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file in nwm_files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    nwm_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Convert to datetime using correct format for these columns\n",
    "    nwm_df['model_initialization_time'] = pd.to_datetime(nwm_df['model_initialization_time'], format='%Y-%m-%d_%H:%M:%S')\n",
    "    nwm_df['model_output_valid_time'] = pd.to_datetime(nwm_df['model_output_valid_time'], format='%Y-%m-%d_%H:%M:%S')\n",
    "    \n",
    "    # Calculate lead time in hours (needed for analysis)\n",
    "    nwm_df['lead_time'] = (nwm_df['model_output_valid_time'] - nwm_df['model_initialization_time']).dt.total_seconds() / 3600\n",
    "    \n",
    "    return nwm_df\n",
    "\n",
    "# Load data for each stream\n",
    "data = {}\n",
    "for stream_id in stream_ids:\n",
    "    try:\n",
    "        usgs_df = load_usgs_data(stream_id)\n",
    "        nwm_df = load_nwm_data(stream_id)\n",
    "        data[stream_id] = {\n",
    "            \"usgs\": usgs_df,\n",
    "            \"nwm\": nwm_df\n",
    "        }\n",
    "        print(f\"Successfully loaded data for stream {stream_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for stream {stream_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca060e",
   "metadata": {},
   "source": [
    "## 2. Examining Data Structure\n",
    "\n",
    "Let's take a look at the structure and contents of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream ID to examine\n",
    "stream_id = stream_ids[0]  # First stream\n",
    "\n",
    "print(\"USGS Data Structure:\")\n",
    "print(data[stream_id][\"usgs\"].head())\n",
    "print(\"\\nUSGS Data Info:\")\n",
    "print(data[stream_id][\"usgs\"].info())\n",
    "print(\"\\nUSGS Data Statistics:\")\n",
    "print(data[stream_id][\"usgs\"].describe())\n",
    "\n",
    "print(\"\\n\\nNWM Data Structure:\")\n",
    "print(data[stream_id][\"nwm\"].head())\n",
    "print(\"\\nNWM Data Info:\")\n",
    "print(data[stream_id][\"nwm\"].info())\n",
    "print(\"\\nNWM Data Statistics by Lead Time:\")\n",
    "print(data[stream_id][\"nwm\"].groupby(\"lead_time\")[\"streamflow_value\"].describe().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363a262",
   "metadata": {},
   "source": [
    "## 3. Temporal Coverage and Availability\n",
    "\n",
    "Let's examine the temporal coverage and availability of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_id in stream_ids:\n",
    "    usgs_df = data[stream_id][\"usgs\"]\n",
    "    nwm_df = data[stream_id][\"nwm\"]\n",
    "    \n",
    "    print(f\"Stream {stream_id} - USGS Data:\")\n",
    "    print(f\"Start date: {usgs_df.index.min()}\")\n",
    "    print(f\"End date: {usgs_df.index.max()}\")\n",
    "    print(f\"Total records: {len(usgs_df)}\")\n",
    "    print(f\"Missing values: {usgs_df.isna().sum().sum()}\")\n",
    "    \n",
    "    print(f\"\\nStream {stream_id} - NWM Data:\")\n",
    "    print(f\"Start reference time: {nwm_df['model_initialization_time'].min()}\")\n",
    "    print(f\"End reference time: {nwm_df['model_initialization_time'].max()}\")\n",
    "    print(f\"Start value time: {nwm_df['model_output_valid_time'].min()}\")\n",
    "    print(f\"End value time: {nwm_df['model_output_valid_time'].max()}\")\n",
    "    print(f\"Total records: {len(nwm_df)}\")\n",
    "    print(f\"Missing values: {nwm_df.isna().sum().sum()}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebd59c",
   "metadata": {},
   "source": [
    "## 4. Data Distribution Analysis\n",
    "\n",
    "Let's visualize the distribution of runoff values in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "for i, stream_id in enumerate(stream_ids):\n",
    "    usgs_df = data[stream_id][\"usgs\"]\n",
    "    nwm_df = data[stream_id][\"nwm\"]\n",
    "    \n",
    "    # Histogram of USGS observed runoff (updated column name)\n",
    "    ax = axes[i, 0]\n",
    "    ax.hist(usgs_df['USGSFlowValue'], bins=50, alpha=0.7)\n",
    "    ax.set_title(f\"Stream {stream_id} - USGS Observed Runoff Distribution\")\n",
    "    ax.set_xlabel(\"Runoff\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    \n",
    "    # Histogram of NWM forecast runoff by lead time (updated column name)\n",
    "    ax = axes[i, 1]\n",
    "    lead_times = [1, 6, 12, 18]  # Representative lead times\n",
    "    for lead in lead_times:\n",
    "        lead_data = nwm_df[nwm_df['lead_time'] == lead]['streamflow_value']\n",
    "        ax.hist(lead_data, bins=50, alpha=0.4, label=f\"Lead {lead}h\")\n",
    "    \n",
    "    ax.set_title(f\"Stream {stream_id} - NWM Forecast Runoff Distribution by Lead Time\")\n",
    "    ax.set_xlabel(\"Runoff\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ed814",
   "metadata": {},
   "source": [
    "## 5. Time Series Alignment\n",
    "\n",
    "Let's restructure the NWM data to align with USGS observations based on valid times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure NWM data to align with USGS observations\n",
    "def align_nwm_with_usgs(stream_id, lead_times=[1, 6, 12, 18]):\n",
    "    usgs_df = data[stream_id][\"usgs\"]\n",
    "    nwm_df = data[stream_id][\"nwm\"]\n",
    "    \n",
    "    aligned_data = {}\n",
    "    # Using the correct column name for USGS flow data\n",
    "    aligned_data['usgs'] = usgs_df['USGSFlowValue']\n",
    "    \n",
    "    for lead in lead_times:\n",
    "        # Filter for this lead time (we calculated this from timestamps)\n",
    "        lead_df = nwm_df[nwm_df['lead_time'] == lead].copy()\n",
    "        # Set index to valid time (when the forecast is for)\n",
    "        lead_df.set_index('model_output_valid_time', inplace=True)\n",
    "        # Get the streamflow column using correct name\n",
    "        lead_series = lead_df['streamflow_value']\n",
    "        # Add to aligned data with a descriptive name\n",
    "        aligned_data[f'nwm_lead_{lead}'] = lead_series\n",
    "    \n",
    "    # Combine into one DataFrame\n",
    "    return pd.DataFrame(aligned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aligned data for visualization\n",
    "for stream_id in stream_ids:\n",
    "    aligned_df = align_nwm_with_usgs(stream_id)\n",
    "    \n",
    "    # Check how much data we have before and after alignment\n",
    "    usgs_count = len(data[stream_id][\"usgs\"])\n",
    "    aligned_count = len(aligned_df.dropna())\n",
    "    data_retention = (aligned_count / usgs_count) * 100 if usgs_count > 0 else 0\n",
    "    \n",
    "    print(f\"Stream {stream_id} - Data retention after alignment: {data_retention:.2f}% ({aligned_count}/{usgs_count} rows)\")\n",
    "    \n",
    "    # Plot time series for a 3-month period\n",
    "    sample_start = pd.Timestamp('2022-01-01')\n",
    "    sample_end = pd.Timestamp('2022-04-01')\n",
    "    sample_df = aligned_df.loc[sample_start:sample_end]\n",
    "    \n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.plot(sample_df.index, sample_df['usgs'], label='USGS Observed', linewidth=2)\n",
    "    for lead in [1, 6, 12, 18]:\n",
    "        if f'nwm_lead_{lead}' in sample_df.columns:\n",
    "            plt.plot(sample_df.index, sample_df[f'nwm_lead_{lead}'], \n",
    "                     label=f'NWM Lead {lead}h', alpha=0.7)\n",
    "    \n",
    "    plt.title(f\"Stream {stream_id} - Observed vs. Forecast Runoff (Jan-Mar 2022)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Runoff\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6daf7",
   "metadata": {},
   "source": [
    "## 6. Forecast Error Analysis\n",
    "\n",
    "Let's calculate and visualize the errors in the NWM forecasts compared to USGS observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d849392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forecast errors\n",
    "for stream_id in stream_ids:\n",
    "    aligned_df = align_nwm_with_usgs(stream_id, lead_times=range(1, 19))\n",
    "    \n",
    "    # Add error columns (forecast - observed)\n",
    "    for lead in range(1, 19):\n",
    "        if f'nwm_lead_{lead}' in aligned_df.columns:\n",
    "            aligned_df[f'error_lead_{lead}'] = aligned_df[f'nwm_lead_{lead}'] - aligned_df['usgs']\n",
    "    \n",
    "    # Plot error distributions for selected lead times\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    lead_times = [1, 6, 12, 18]\n",
    "    for i, lead in enumerate(lead_times):\n",
    "        if f'error_lead_{lead}' in aligned_df.columns:\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            sns.histplot(aligned_df[f'error_lead_{lead}'].dropna(), kde=True)\n",
    "            plt.title(f\"Stream {stream_id} - Error Distribution (Lead {lead}h)\")\n",
    "            plt.xlabel(\"Forecast Error (NWM - USGS)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot error vs lead time\n",
    "    error_means = [aligned_df[f'error_lead_{lead}'].mean() for lead in range(1, 19) if f'error_lead_{lead}' in aligned_df.columns]\n",
    "    error_stds = [aligned_df[f'error_lead_{lead}'].std() for lead in range(1, 19) if f'error_lead_{lead}' in aligned_df.columns]\n",
    "    lead_list = [lead for lead in range(1, 19) if f'error_lead_{lead}' in aligned_df.columns]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.errorbar(lead_list, error_means, yerr=error_stds, fmt='o-')\n",
    "    plt.title(f\"Stream {stream_id} - Mean Forecast Error by Lead Time\")\n",
    "    plt.xlabel(\"Lead Time (hours)\")\n",
    "    plt.ylabel(\"Mean Error (NWM - USGS)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbdf1b2",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis\n",
    "\n",
    "Let's investigate the correlation between observed values and forecasts at different lead times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "for stream_id in stream_ids:\n",
    "    aligned_df = align_nwm_with_usgs(stream_id, lead_times=range(1, 19))\n",
    "    \n",
    "    correlations = []\n",
    "    lead_times = []\n",
    "    \n",
    "    for lead in range(1, 19):\n",
    "        col = f'nwm_lead_{lead}'\n",
    "        if col in aligned_df.columns:\n",
    "            # Drop any NaN values\n",
    "            valid_data = aligned_df[['usgs', col]].dropna()\n",
    "            if len(valid_data) > 0:\n",
    "                corr, _ = stats.pearsonr(valid_data['usgs'], valid_data[col])\n",
    "                correlations.append(corr)\n",
    "                lead_times.append(lead)\n",
    "    \n",
    "    # Plot correlation vs lead time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(lead_times, correlations, 'o-')\n",
    "    plt.title(f\"Stream {stream_id} - Correlation between Observed and Forecast Runoff\")\n",
    "    plt.xlabel(\"Lead Time (hours)\")\n",
    "    plt.ylabel(\"Pearson Correlation Coefficient\")\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be145656",
   "metadata": {},
   "source": [
    "## 8. Seasonal Patterns in Forecast Errors\n",
    "\n",
    "Let's investigate if there are seasonal patterns in the forecast errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab23871",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_id in stream_ids:\n",
    "    aligned_df = align_nwm_with_usgs(stream_id)\n",
    "    \n",
    "    # Add month column\n",
    "    aligned_df['month'] = aligned_df.index.month\n",
    "    \n",
    "    # Calculate mean errors by month for different lead times\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, lead in enumerate([1, 6, 12, 18]):\n",
    "        if f'error_lead_{lead}' in aligned_df.columns:\n",
    "            monthly_errors = aligned_df.groupby('month')[f'error_lead_{lead}'].mean()\n",
    "            plt.plot(monthly_errors.index, monthly_errors.values, 'o-', \n",
    "                     label=f'Lead {lead}h')\n",
    "    \n",
    "    plt.title(f\"Stream {stream_id} - Monthly Mean Forecast Error\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Mean Error (NWM - USGS)\")\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                             'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d57ac",
   "metadata": {},
   "source": [
    "## 9. Autocorrelation Analysis of Forecast Errors\n",
    "\n",
    "Let's investigate the autocorrelation in forecast errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc875be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "for stream_id in stream_ids:\n",
    "    aligned_df = align_nwm_with_usgs(stream_id)\n",
    "    \n",
    "    # Add error columns if they don't exist\n",
    "    for lead in [1, 6, 12, 18]:\n",
    "        if f'nwm_lead_{lead}' in aligned_df.columns and f'error_lead_{lead}' not in aligned_df.columns:\n",
    "            aligned_df[f'error_lead_{lead}'] = aligned_df[f'nwm_lead_{lead}'] - aligned_df['usgs']\n",
    "    \n",
    "    # Plot autocorrelation for errors at different lead times\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, lead in enumerate([1, 6, 12, 18]):\n",
    "        if f'error_lead_{lead}' in aligned_df.columns:\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            autocorrelation_plot(aligned_df[f'error_lead_{lead}'].dropna())\n",
    "            plt.title(f\"Stream {stream_id} - Error Autocorrelation (Lead {lead}h)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d34cb",
   "metadata": {},
   "source": [
    "## 10. Error Persistence Analysis\n",
    "\n",
    "Let's analyze whether errors are persistent across lead times, which would indicate potential advantages for the baseline persistence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95cee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_id in stream_ids:\n",
    "    aligned_df = align_nwm_with_usgs(stream_id, lead_times=range(1, 19))\n",
    "    \n",
    "    # Calculate errors\n",
    "    for lead in range(1, 19):\n",
    "        col = f'nwm_lead_{lead}'\n",
    "        if col in aligned_df.columns and f'error_lead_{lead}' not in aligned_df.columns:\n",
    "            aligned_df[f'error_lead_{lead}'] = aligned_df[col] - aligned_df['usgs']\n",
    "    \n",
    "    # Calculate correlation between errors at different lead times\n",
    "    error_cols = [col for col in aligned_df.columns if col.startswith('error_lead_')]\n",
    "    if error_cols:\n",
    "        error_corr = aligned_df[error_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(error_corr, annot=True, cmap='coolwarm', fmt='.2f', \n",
    "                   linewidths=0.5, square=True)\n",
    "        plt.title(f\"Stream {stream_id} - Error Correlation Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac95c1",
   "metadata": {},
   "source": [
    "## 11. Flow Regime Analysis\n",
    "\n",
    "Let's examine how forecast errors vary across different flow regimes (low, medium, high flows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f271653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_id in stream_ids:\n",
    "    aligned_df = align_nwm_with_usgs(stream_id)\n",
    "    \n",
    "    # Add flow regime classification based on quantiles\n",
    "    flow_quantiles = aligned_df['usgs'].quantile([0.25, 0.75]).values\n",
    "    aligned_df['flow_regime'] = 'Medium'\n",
    "    aligned_df.loc[aligned_df['usgs'] <= flow_quantiles[0], 'flow_regime'] = 'Low'\n",
    "    aligned_df.loc[aligned_df['usgs'] >= flow_quantiles[1], 'flow_regime'] = 'High'\n",
    "    \n",
    "    # Make sure error columns exist\n",
    "    lead_times = [1, 6, 12, 18]\n",
    "    for lead in lead_times:\n",
    "        if f'nwm_lead_{lead}' in aligned_df.columns and f'error_lead_{lead}' not in aligned_df.columns:\n",
    "            aligned_df[f'error_lead_{lead}'] = aligned_df[f'nwm_lead_{lead}'] - aligned_df['usgs']\n",
    "    \n",
    "    # Create boxplots of errors by flow regime\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, lead in enumerate(lead_times):\n",
    "        if f'error_lead_{lead}' in aligned_df.columns:\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            error_by_regime = [aligned_df[aligned_df['flow_regime'] == regime][f'error_lead_{lead}'].dropna() \n",
    "                              for regime in ['Low', 'Medium', 'High']]\n",
    "            \n",
    "            plt.boxplot(error_by_regime, labels=['Low', 'Medium', 'High'], showfliers=False)\n",
    "            plt.title(f\"Stream {stream_id} - Forecast Error by Flow Regime (Lead {lead}h)\")\n",
    "            plt.xlabel(\"Flow Regime\")\n",
    "            plt.ylabel(\"Forecast Error (NWM - USGS)\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print error statistics by flow regime\n",
    "    print(f\"Error Statistics by Flow Regime for Stream {stream_id}:\")\n",
    "    for lead in lead_times:\n",
    "        if f'error_lead_{lead}' in aligned_df.columns:\n",
    "            print(f\"\\nLead Time: {lead} hours\")\n",
    "            for regime in ['Low', 'Medium', 'High']:\n",
    "                regime_errors = aligned_df[aligned_df['flow_regime'] == regime][f'error_lead_{lead}'].dropna()\n",
    "                if len(regime_errors) > 0:\n",
    "                    print(f\"  {regime} Flow Regime:\")\n",
    "                    print(f\"    Mean Error: {regime_errors.mean():.2f}\")\n",
    "                    print(f\"    RMSE: {np.sqrt(np.mean(regime_errors**2)):.2f}\")\n",
    "                    print(f\"    % Bias: {100 * regime_errors.mean() / aligned_df[aligned_df['flow_regime'] == regime]['usgs'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1690e3",
   "metadata": {},
   "source": [
    "## 12. Data Alignment Analysis\n",
    "\n",
    "The README mentions a significant data loss (~75%) during alignment. Let's investigate the causes of this data loss to better understand the challenges in data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_id in stream_ids:\n",
    "    usgs_df = data[stream_id][\"usgs\"]\n",
    "    nwm_df = data[stream_id][\"nwm\"]\n",
    "    \n",
    "    # Check for missing data by time pattern\n",
    "    usgs_by_hour = usgs_df.groupby(usgs_df.index.hour).count()['USGSFlowValue']  # Using correct column name\n",
    "    nwm_by_hour = nwm_df.groupby(nwm_df['model_output_valid_time'].dt.hour).count()['streamflow_value']  # Using correct column name\n",
    "    \n",
    "    # Plot missing data patterns by hour of day\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=usgs_by_hour.index, y=usgs_by_hour.values)\n",
    "    plt.title(f\"Stream {stream_id} - USGS Data Count by Hour\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Data Count\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=nwm_by_hour.index, y=nwm_by_hour.values)\n",
    "    plt.title(f\"Stream {stream_id} - NWM Data Count by Hour\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Data Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Count NWM data by lead time\n",
    "    nwm_by_lead = nwm_df.groupby('lead_time').count()['streamflow_value']  # Using correct column name\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=nwm_by_lead.index.astype(int), y=nwm_by_lead.values)\n",
    "    plt.title(f\"Stream {stream_id} - NWM Data Count by Lead Time\")\n",
    "    plt.xlabel(\"Lead Time (hours)\")\n",
    "    plt.ylabel(\"Data Count\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate percentage of aligned data\n",
    "    usgs_timestamps = set(usgs_df.index)\n",
    "    nwm_timestamps = {}\n",
    "    \n",
    "    for lead in range(1, 19):\n",
    "        lead_df = nwm_df[nwm_df['lead_time'] == lead]\n",
    "        if not lead_df.empty:\n",
    "            nwm_timestamps[lead] = set(lead_df['model_output_valid_time'])\n",
    "    \n",
    "    # Calculate intersection for each lead time\n",
    "    for lead in range(1, 19):\n",
    "        if lead in nwm_timestamps:\n",
    "            intersection = usgs_timestamps.intersection(nwm_timestamps[lead])\n",
    "            if nwm_timestamps[lead]:\n",
    "                match_percent = len(intersection) / len(usgs_timestamps) * 100\n",
    "                print(f\"Stream {stream_id} - Lead {lead}h: {match_percent:.1f}% of USGS timestamps have matching NWM forecasts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f9bcd",
   "metadata": {},
   "source": [
    "## 13. Summary of Findings and Implications for Model Development\n",
    "\n",
    "Based on our exploratory data analysis, here is a summary of key findings and their implications for the Seq2Seq LSTM model development:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942304a",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Data Coverage and Alignment:**\n",
    "   - Significant data loss during alignment between NWM forecasts and USGS observations\n",
    "   - Temporal patterns in data availability may affect model training\n",
    "   - NWM forecasts show varying availability across different lead times\n",
    "\n",
    "2. **Error Distribution and Characteristics:**\n",
    "   - Forecast errors increase with lead time, as expected in hydrological forecasting\n",
    "   - Error distributions tend to be positively skewed (NWM often overpredicts runoff)\n",
    "   - Strong autocorrelation in forecast errors suggests potential for the LSTM to leverage temporal patterns\n",
    "\n",
    "3. **Flow Regime Analysis:**\n",
    "   - Error magnitude and bias vary significantly across different flow regimes (low, medium, high)\n",
    "   - High flows show larger absolute errors but potentially smaller relative errors\n",
    "   - The model may need to handle these regime-dependent error patterns\n",
    "\n",
    "4. **Seasonal Patterns:**\n",
    "   - Clear seasonal patterns in forecast errors suggest the importance of capturing seasonal dependencies\n",
    "   - These patterns differ by lead time, indicating potentially complex temporal dynamics\n",
    "\n",
    "5. **Error Persistence and Correlation:**\n",
    "   - Strong correlation between errors at consecutive lead times\n",
    "   - Error autocorrelation suggests past errors contain valuable information for future error prediction\n",
    "   - The persistence baseline model likely benefits from these correlations\n",
    "\n",
    "### Implications for Seq2Seq LSTM Model Development\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Include lagged values of observed runoff, forecasts, and errors as they contain valuable information\n",
    "   - Consider adding explicit seasonal features (e.g., month, day of year) to help capture seasonal patterns\n",
    "   - Flow regime indicators might help the model adapt to different error patterns across flow magnitudes\n",
    "\n",
    "2. **Model Architecture:**\n",
    "   - The sequence length should be sufficient to capture relevant temporal dependencies (24 hours seems reasonable based on autocorrelation analysis)\n",
    "   - The strong correlation between errors at different lead times supports the simultaneous prediction of all lead times (1-18h)\n",
    "   - Consider using attention mechanisms to allow the model to focus on the most relevant parts of the input sequence\n",
    "\n",
    "3. **Training Considerations:**\n",
    "   - The positively skewed error distributions suggest that a custom loss function might be important\n",
    "   - Different weighting schemes might be explored to balance performance across flow regimes\n",
    "   - TimeSeriesSplit cross-validation is crucial given the strong temporal dependencies in the data\n",
    "\n",
    "4. **Evaluation Strategy:**\n",
    "   - Evaluate model performance separately for different flow regimes and seasons\n",
    "   - Compare against both the raw NWM forecasts and the persistence baseline\n",
    "   - Use multiple metrics (CC, RMSE, PBIAS, NSE) as they provide complementary information about model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
