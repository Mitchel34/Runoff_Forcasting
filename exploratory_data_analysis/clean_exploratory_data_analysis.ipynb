{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68a808e",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Runoff Forecasting\n",
    "\n",
    "This notebook explores the USGS observed runoff data and NWM forecast data to prepare for an LSTM-based runoff forecasting model.\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "**USGS Data:**\n",
    "* Each stream has one CSV file with hourly observations\n",
    "* File naming pattern: `*_Strt_*.csv`\n",
    "* Columns:\n",
    "  * DateTime: Timestamp with timezone (e.g., \"2021-04-20 07:00:00+00:00\")\n",
    "  * USGSFlowValue: The observed runoff value\n",
    "  * USGS_GageID or 00060_cd: A quality code (mostly \"A\")\n",
    "\n",
    "**NWM Data:**\n",
    "* Each stream has 25 monthly CSV files with forecasts\n",
    "* File naming pattern: `streamflow_[streamID]_[YYYYMM].csv`\n",
    "* Columns:\n",
    "  * NWM_version_number: Version of the NWM model (e.g., \"v2.1\")\n",
    "  * model_initialization_time: When the forecast was issued\n",
    "  * model_output_valid_time: When the forecast is valid for\n",
    "  * streamflow_value: The predicted runoff value\n",
    "  * streamID: The stream identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646aa66",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Increase figure resolution for better clarity\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6765f3",
   "metadata": {},
   "source": [
    "## Helper Functions for Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_usgs_data(folder_path):\n",
    "    \"\"\"Load USGS observation data for a given stream.\"\"\"\n",
    "    # Find the USGS file using pattern matching\n",
    "    usgs_files = glob.glob(os.path.join(folder_path, '*_Strt_*.csv'))\n",
    "    \n",
    "    if not usgs_files:\n",
    "        print(f\"No USGS data files found in {folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load the first matching file\n",
    "    usgs_file = usgs_files[0]\n",
    "    print(f\"Loading USGS data from: {usgs_file}\")\n",
    "    \n",
    "    # Extract USGS station ID from filename\n",
    "    station_id = os.path.basename(usgs_file).split('_')[0]\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(usgs_file)\n",
    "    \n",
    "    # Convert datetime column\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    \n",
    "    return df, station_id\n",
    "\n",
    "def load_nwm_data(folder_path, stream_id):\n",
    "    \"\"\"Load and combine all NWM forecast data for a given stream.\"\"\"\n",
    "    # Find all NWM files for this stream\n",
    "    nwm_files = glob.glob(os.path.join(folder_path, f\"streamflow_{stream_id}_*.csv\"))\n",
    "    \n",
    "    if not nwm_files:\n",
    "        print(f\"No NWM data files found for stream {stream_id} in {folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(nwm_files)} NWM data files for stream {stream_id}\")\n",
    "    \n",
    "    # Load and combine all files\n",
    "    dfs = []\n",
    "    for file in sorted(nwm_files):\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Combine all monthly dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['model_initialization_time', 'model_output_valid_time']\n",
    "    for col in datetime_cols:\n",
    "        if col in combined_df.columns:\n",
    "            combined_df[col] = pd.to_datetime(combined_df[col])\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def get_stream_folders():\n",
    "    \"\"\"Get all stream folders in the current directory.\"\"\"\n",
    "    # List all directories in the current directory\n",
    "    stream_folders = [d for d in os.listdir() if os.path.isdir(d) and d.isdigit()]\n",
    "    return stream_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2e858",
   "metadata": {},
   "source": [
    "## Explore Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all stream folders\n",
    "stream_folders = get_stream_folders()\n",
    "print(f\"Found {len(stream_folders)} stream folders: {stream_folders}\")\n",
    "\n",
    "# Look at file structure in the first folder\n",
    "if stream_folders:\n",
    "    first_folder = stream_folders[0]\n",
    "    files = os.listdir(first_folder)\n",
    "    \n",
    "    # Count different file types\n",
    "    usgs_files = [f for f in files if \"_Strt_\" in f]\n",
    "    nwm_files = [f for f in files if \"streamflow_\" in f]\n",
    "    \n",
    "    print(f\"\\nIn folder {first_folder}:\")\n",
    "    print(f\"- USGS observation files: {len(usgs_files)}\")\n",
    "    print(f\"- NWM forecast files: {len(nwm_files)}\")\n",
    "    \n",
    "    # Show file naming patterns\n",
    "    if usgs_files:\n",
    "        print(f\"\\nExample USGS file: {usgs_files[0]}\")\n",
    "    if nwm_files:\n",
    "        print(f\"Example NWM files: {nwm_files[:3]} ...\")\n",
    "        \n",
    "        # Extract date range from NWM files\n",
    "        dates = [re.search(r'_(\\d{6})\\.csv$', f).group(1) for f in nwm_files if re.search(r'_(\\d{6})\\.csv$', f)]\n",
    "        dates = [f\"{d[:4]}-{d[4:]}\" for d in dates]\n",
    "        dates.sort()\n",
    "        if dates:\n",
    "            print(f\"NWM data spans from {dates[0]} to {dates[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d227c4",
   "metadata": {},
   "source": [
    "## Load and Examine USGS Observation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first stream for initial analysis\n",
    "stream_folder = stream_folders[0]\n",
    "usgs_data, station_id = load_usgs_data(stream_folder)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nUSGS Data Overview for Station {station_id}:\")\n",
    "print(f\"Shape: {usgs_data.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "print(usgs_data.columns.tolist())\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(usgs_data.head())\n",
    "\n",
    "# Get data statistics\n",
    "print(\"\\nUSGS Flow Value Statistics:\")\n",
    "display(usgs_data['USGSFlowValue'].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(usgs_data.isnull().sum())\n",
    "\n",
    "# Check date range\n",
    "date_range = usgs_data['DateTime'].agg(['min', 'max'])\n",
    "print(f\"\\nDate range: {date_range['min']} to {date_range['max']}\")\n",
    "print(f\"Total duration: {date_range['max'] - date_range['min']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f0a99",
   "metadata": {},
   "source": [
    "## Visualize USGS Observation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484efac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot of USGS flow values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(usgs_data['DateTime'], usgs_data['USGSFlowValue'], color='blue', linewidth=1)\n",
    "plt.title(f'USGS Flow Values Over Time for Station {station_id}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Flow Value (cfs)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of flow values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(usgs_data['USGSFlowValue'], kde=True, bins=30)\n",
    "plt.title('Distribution of Flow Values')\n",
    "plt.xlabel('Flow Value (cfs)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(np.log1p(usgs_data['USGSFlowValue']), kde=True, bins=30)\n",
    "plt.title('Distribution of Log-Transformed Flow Values')\n",
    "plt.xlabel('Log(Flow Value + 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Examine flow code distribution if available\n",
    "quality_code_col = next((col for col in usgs_data.columns \n",
    "                         if 'code' in col.lower() or 'GageID' in col), None)\n",
    "\n",
    "if quality_code_col:\n",
    "    code_counts = usgs_data[quality_code_col].value_counts()\n",
    "    print(f\"\\nDistribution of quality codes ({quality_code_col}):\")\n",
    "    display(code_counts)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(y=usgs_data[quality_code_col])\n",
    "    plt.title('Distribution of Quality Codes')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058d1fb",
   "metadata": {},
   "source": [
    "## Load and Examine NWM Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eda557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stream ID from the folder name\n",
    "stream_id = stream_folder\n",
    "nwm_data = load_nwm_data(stream_folder, stream_id)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nNWM Data Overview for Stream {stream_id}:\")\n",
    "print(f\"Shape: {nwm_data.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "print(nwm_data.columns.tolist())\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(nwm_data.head())\n",
    "\n",
    "# Get data statistics\n",
    "print(\"\\nNWM Flow Value Statistics:\")\n",
    "display(nwm_data['streamflow_value'].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(nwm_data.isnull().sum())\n",
    "\n",
    "# Check unique NWM versions\n",
    "if 'NWM_version_number' in nwm_data.columns:\n",
    "    versions = nwm_data['NWM_version_number'].unique()\n",
    "    print(f\"\\nNWM model versions: {versions}\")\n",
    "\n",
    "# Check date range\n",
    "if 'model_output_valid_time' in nwm_data.columns:\n",
    "    date_range = nwm_data['model_output_valid_time'].agg(['min', 'max'])\n",
    "    print(f\"\\nForecast valid time range: {date_range['min']} to {date_range['max']}\")\n",
    "    print(f\"Total duration: {date_range['max'] - date_range['min']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e527d0",
   "metadata": {},
   "source": [
    "## Visualize NWM Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot of NWM flow values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(nwm_data['model_output_valid_time'], nwm_data['streamflow_value'], \n",
    "         color='green', linewidth=1, alpha=0.7)\n",
    "plt.title(f'NWM Forecast Flow Values Over Time for Stream {stream_id}')\n",
    "plt.xlabel('Valid Time')\n",
    "plt.ylabel('Flow Value (cfs)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of flow values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(nwm_data['streamflow_value'], kde=True, bins=30)\n",
    "plt.title('Distribution of NWM Flow Values')\n",
    "plt.xlabel('Flow Value (cfs)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(np.log1p(nwm_data['streamflow_value']), kde=True, bins=30)\n",
    "plt.title('Distribution of Log-Transformed NWM Flow Values')\n",
    "plt.xlabel('Log(Flow Value + 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Examine forecast lead times\n",
    "if 'model_initialization_time' in nwm_data.columns and 'model_output_valid_time' in nwm_data.columns:\n",
    "    # Calculate lead time in hours\n",
    "    nwm_data['lead_time_hours'] = (nwm_data['model_output_valid_time'] - \n",
    "                                  nwm_data['model_initialization_time']).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Plot distribution of lead times\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(nwm_data['lead_time_hours'], bins=30, kde=True)\n",
    "    plt.title('Distribution of Forecast Lead Times')\n",
    "    plt.xlabel('Lead Time (hours)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show common lead times\n",
    "    common_lead_times = nwm_data['lead_time_hours'].value_counts().head(10)\n",
    "    print(\"\\nMost common forecast lead times (hours):\")\n",
    "    display(common_lead_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e55451b",
   "metadata": {},
   "source": [
    "## Compare USGS Observations with NWM Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for comparison\n",
    "# We'll use the NWM forecasts with the shortest lead time for each valid time point\n",
    "\n",
    "# First, make sure the USGS data has a datetime index for easy alignment\n",
    "usgs_aligned = usgs_data.copy()\n",
    "usgs_aligned.set_index('DateTime', inplace=True)\n",
    "\n",
    "# For NWM data, select the shortest lead time for each valid time\n",
    "if 'lead_time_hours' in nwm_data.columns:\n",
    "    # Group by valid time and select the entry with shortest lead time\n",
    "    nwm_shortest_leads = nwm_data.loc[nwm_data.groupby('model_output_valid_time')['lead_time_hours'].idxmin()]\n",
    "    \n",
    "    # Set the index to the valid time for alignment\n",
    "    nwm_aligned = nwm_shortest_leads.set_index('model_output_valid_time')\n",
    "    \n",
    "    # Align the time series and create a combined dataframe\n",
    "    combined = pd.DataFrame({\n",
    "        'USGS_Flow': usgs_aligned['USGSFlowValue'],\n",
    "        'NWM_Flow': nwm_aligned['streamflow_value']\n",
    "    })\n",
    "    \n",
    "    # Plot both time series\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(combined.index, combined['USGS_Flow'], label='USGS Observed', color='blue', linewidth=1.5)\n",
    "    plt.plot(combined.index, combined['NWM_Flow'], label='NWM Forecast', color='green', linewidth=1, alpha=0.7)\n",
    "    plt.title(f'Comparison of USGS Observed vs NWM Forecast Flow Values for Stream {stream_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Flow Value (cfs)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a scatter plot to visualize the relationship\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Drop any rows with missing values\n",
    "    combined_clean = combined.dropna()\n",
    "    \n",
    "    plt.scatter(combined_clean['USGS_Flow'], combined_clean['NWM_Flow'], \n",
    "                alpha=0.5, edgecolor='k', linewidth=0.5)\n",
    "    \n",
    "    # Add a perfect prediction line\n",
    "    max_val = max(combined_clean['USGS_Flow'].max(), combined_clean['NWM_Flow'].max())\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--', linewidth=1)\n",
    "    \n",
    "    plt.title('USGS Observed vs NWM Forecast Flow Values')\n",
    "    plt.xlabel('USGS Observed Flow (cfs)')\n",
    "    plt.ylabel('NWM Forecast Flow (cfs)')\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation and error metrics\n",
    "    correlation = combined_clean.corr().iloc[0, 1]\n",
    "    mae = np.mean(np.abs(combined_clean['USGS_Flow'] - combined_clean['NWM_Flow']))\n",
    "    rmse = np.sqrt(np.mean((combined_clean['USGS_Flow'] - combined_clean['NWM_Flow'])**2))\n",
    "    mape = np.mean(np.abs((combined_clean['USGS_Flow'] - combined_clean['NWM_Flow']) / combined_clean['USGS_Flow'])) * 100\n",
    "    \n",
    "    print(f\"Correlation between USGS and NWM: {correlation:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f} cfs\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f} cfs\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}%\")\n",
    "    \n",
    "    # Also calculate log-space metrics to reduce influence of extreme values\n",
    "    print(\"\\nLog-space metrics:\")\n",
    "    combined_clean_log = np.log1p(combined_clean)\n",
    "    log_correlation = combined_clean_log.corr().iloc[0, 1]\n",
    "    log_mae = np.mean(np.abs(combined_clean_log['USGS_Flow'] - combined_clean_log['NWM_Flow']))\n",
    "    log_rmse = np.sqrt(np.mean((combined_clean_log['USGS_Flow'] - combined_clean_log['NWM_Flow'])**2))\n",
    "    \n",
    "    print(f\"Log-space Correlation: {log_correlation:.4f}\")\n",
    "    print(f\"Log-space MAE: {log_mae:.4f}\")\n",
    "    print(f\"Log-space RMSE: {log_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24211bba",
   "metadata": {},
   "source": [
    "## Time Series Analysis and Feature Engineering for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct time series analysis on USGS data (observed values)\n",
    "# We'll resample to daily data for seasonal decomposition\n",
    "usgs_daily = usgs_aligned['USGSFlowValue'].resample('D').mean()\n",
    "\n",
    "# Plot the resampled daily time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "usgs_daily.plot()\n",
    "plt.title('Daily Average USGS Flow Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Flow Value (cfs)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal Decomposition\n",
    "try:\n",
    "    # Fill any missing values for decomposition\n",
    "    usgs_daily_filled = usgs_daily.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Perform decomposition if we have enough data\n",
    "    if len(usgs_daily_filled) >= 2*365:  # At least 2 years for good decomposition\n",
    "        decomposition = seasonal_decompose(usgs_daily_filled, model='additive', period=365)\n",
    "        \n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 12))\n",
    "        decomposition.observed.plot(ax=ax1)\n",
    "        ax1.set_title('Observed')\n",
    "        ax1.set_ylabel('Flow Value')\n",
    "        \n",
    "        decomposition.trend.plot(ax=ax2)\n",
    "        ax2.set_title('Trend')\n",
    "        ax2.set_ylabel('Flow Value')\n",
    "        \n",
    "        decomposition.seasonal.plot(ax=ax3)\n",
    "        ax3.set_title('Seasonality')\n",
    "        ax3.set_ylabel('Flow Value')\n",
    "        \n",
    "        decomposition.resid.plot(ax=ax4)\n",
    "        ax4.set_title('Residuals')\n",
    "        ax4.set_ylabel('Flow Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough data for seasonal decomposition (need at least 2 years)\")\n",
    "except Exception as e:\n",
    "    print(f\"Seasonal decomposition failed: {e}\")\n",
    "\n",
    "# Autocorrelation and Partial Autocorrelation Analysis\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_acf(usgs_daily.dropna(), lags=40, ax=plt.gca())\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_pacf(usgs_daily.dropna(), lags=40, ax=plt.gca())\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7818f",
   "metadata": {},
   "source": [
    "## Feature Engineering Ideas for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee583421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of features that would be useful for LSTM model\n",
    "\n",
    "# We'll use the combined dataframe from earlier\n",
    "if 'combined' in locals():\n",
    "    # Create a copy to avoid modifying the original\n",
    "    features_df = combined.copy()\n",
    "    \n",
    "    # 1. Add time-based features\n",
    "    features_df['hour'] = features_df.index.hour\n",
    "    features_df['day'] = features_df.index.day\n",
    "    features_df['month'] = features_df.index.month\n",
    "    features_df['year'] = features_df.index.year\n",
    "    features_df['day_of_week'] = features_df.index.dayofweek\n",
    "    features_df['day_of_year'] = features_df.index.dayofyear\n",
    "    \n",
    "    # 2. Add lag features (past values)\n",
    "    for lag in [1, 3, 6, 12, 24, 48, 72]:  # Hours\n",
    "        features_df[f'USGS_lag_{lag}h'] = features_df['USGS_Flow'].shift(lag)\n",
    "        features_df[f'NWM_lag_{lag}h'] = features_df['NWM_Flow'].shift(lag)\n",
    "    \n",
    "    # 3. Add rolling statistics\n",
    "    for window in [6, 12, 24, 48, 72]:  # Hours\n",
    "        features_df[f'USGS_rolling_mean_{window}h'] = features_df['USGS_Flow'].rolling(window=window).mean()\n",
    "        features_df[f'USGS_rolling_std_{window}h'] = features_df['USGS_Flow'].rolling(window=window).std()\n",
    "        features_df[f'USGS_rolling_min_{window}h'] = features_df['USGS_Flow'].rolling(window=window).min()\n",
    "        features_df[f'USGS_rolling_max_{window}h'] = features_df['USGS_Flow'].rolling(window=window).max()\n",
    "    \n",
    "    # 4. Difference features (rate of change)\n",
    "    features_df['USGS_diff_1h'] = features_df['USGS_Flow'].diff(1)\n",
    "    features_df['USGS_diff_24h'] = features_df['USGS_Flow'].diff(24)\n",
    "    \n",
    "    # 5. Cyclic encoding of time features (preserves cyclical nature)\n",
    "    features_df['hour_sin'] = np.sin(2 * np.pi * features_df['hour'] / 24)\n",
    "    features_df['hour_cos'] = np.cos(2 * np.pi * features_df['hour'] / 24)\n",
    "    features_df['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
    "    features_df['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
    "    features_df['day_of_year_sin'] = np.sin(2 * np.pi * features_df['day_of_year'] / 365)\n",
    "    features_df['day_of_year_cos'] = np.cos(2 * np.pi * features_df['day_of_year'] / 365)\n",
    "    \n",
    "    # Display the features dataframe\n",
    "    print(\"Feature-engineered DataFrame for LSTM model:\")\n",
    "    display(features_df.head())\n",
    "    \n",
    "    # Show the total number of features\n",
    "    print(f\"\\nTotal number of features: {features_df.shape[1]}\")\n",
    "    \n",
    "    # Correlation of features with the target (future flow)\n",
    "    # Let's say we want to predict 24 hours ahead\n",
    "    target_col = 'USGS_Flow'\n",
    "    features_df['target_24h_ahead'] = features_df[target_col].shift(-24)\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    corr_with_target = features_df.corr()['target_24h_ahead'].sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 features most correlated with the target (24h ahead flow):\")\n",
    "    display(corr_with_target.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08616cb5",
   "metadata": {},
   "source": [
    "## LSTM Model Preparation Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74670caa",
   "metadata": {},
   "source": [
    "Based on the exploratory data analysis, here are the recommended steps for building an LSTM model for runoff forecasting:\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "- **Handling Missing Values**: Fill or interpolate missing values in both USGS and NWM data.\n",
    "- **Feature Scaling**: Normalize or standardize all features to the range [0,1] or having mean 0 and std 1.\n",
    "- **Log Transformation**: Consider log-transforming flow values to reduce skewness (use `np.log1p()`).\n",
    "- **Resampling**: Consider resampling data to a uniform frequency (hourly or daily).\n",
    "\n",
    "### 2. Feature Engineering\n",
    "\n",
    "- **Time-based Features**: Hour, day, month, day of week, day of year.\n",
    "- **Cyclic Encoding**: Sine and cosine transformations of cyclical time features.\n",
    "- **Lag Features**: Past values of flow at different time lags (1h, 6h, 12h, 24h, etc.).\n",
    "- **Rolling Window Statistics**: Mean, min, max, std of past flow values.\n",
    "- **Difference Features**: Rate of change between time steps.\n",
    "- **NWM Forecast Features**: Include NWM forecasts as features.\n",
    "\n",
    "### 3. Sequence Preparation for LSTM\n",
    "\n",
    "- **Sequence Length**: Based on ACF/PACF analysis, use a window of significant lag periods (e.g., 72 hours).\n",
    "- **Sequence Creation**: Create input sequences X (features over time) and target sequences y (future flow values).\n",
    "- **Sequence Format**: Shape should be [samples, time steps, features].\n",
    "\n",
    "### 4. Train-Validation-Test Split\n",
    "\n",
    "- **Chronological Split**: Use earlier data for training, middle for validation, and latest for testing.\n",
    "- **Proportions**: Typically 70% train, 15% validation, 15% test.\n",
    "\n",
    "### 5. LSTM Model Architecture\n",
    "\n",
    "- **Input Layer**: LSTM layer with appropriate input shape.\n",
    "- **Hidden Layers**: Additional LSTM layers with dropout for regularization.\n",
    "- **Output Layer**: Dense layer with linear activation for regression.\n",
    "- **Sample Structure**:\n",
    "  ```python\n",
    "  model = Sequential([\n",
    "      LSTM(50, return_sequences=True, input_shape=(sequence_length, num_features)),\n",
    "      Dropout(0.2),\n",
    "      LSTM(50),\n",
    "      Dropout(0.2),\n",
    "      Dense(1)\n",
    "  ])\n",
    "  ```\n",
    "\n",
    "### 6. Model Training\n",
    "\n",
    "- **Loss Function**: Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n",
    "- **Optimizer**: Adam with appropriate learning rate.\n",
    "- **Early Stopping**: To prevent overfitting.\n",
    "- **Learning Rate Scheduling**: Reduce learning rate when plateauing.\n",
    "\n",
    "### 7. Model Evaluation\n",
    "\n",
    "- **Metrics**: RMSE, MAE, MAPE, and correlation coefficient.\n",
    "- **Visualization**: Compare predicted vs actual values.\n",
    "- **Benchmark**: Compare with simpler models and NWM forecasts.\n",
    "\n",
    "### 8. Multi-Step Forecasting Strategy\n",
    "\n",
    "- **Direct Method**: Train separate models for each forecast horizon.\n",
    "- **Recursive Method**: Use one-step predictions as inputs for next steps.\n",
    "- **Multiple Output**: Train a single model to predict multiple time steps ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cc12f",
   "metadata": {},
   "source": [
    "## Analyze Second Stream for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have multiple streams to compare\n",
    "if len(stream_folders) > 1:\n",
    "    # Select the second stream\n",
    "    second_stream_folder = stream_folders[1]\n",
    "    second_usgs_data, second_station_id = load_usgs_data(second_stream_folder)\n",
    "    \n",
    "    if second_usgs_data is not None:\n",
    "        # Plot the two streams' USGS data for comparison\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Align both datasets to have the same datetime index format\n",
    "        first_stream_ts = usgs_data.set_index('DateTime')['USGSFlowValue']\n",
    "        second_stream_ts = second_usgs_data.set_index('DateTime')['USGSFlowValue']\n",
    "        \n",
    "        # Plot both time series\n",
    "        plt.plot(first_stream_ts.index, first_stream_ts, \n",
    "                 label=f'Stream {station_id}', color='blue', linewidth=1.5)\n",
    "        plt.plot(second_stream_ts.index, second_stream_ts, \n",
    "                 label=f'Stream {second_station_id}', color='red', linewidth=1.5)\n",
    "        \n",
    "        plt.title('Comparison of Runoff Between Two Streams')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Flow Value (cfs)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Normalize the data and plot again to compare patterns\n",
    "        first_stream_norm = (first_stream_ts - first_stream_ts.min()) / (first_stream_ts.max() - first_stream_ts.min())\n",
    "        second_stream_norm = (second_stream_ts - second_stream_ts.min()) / (second_stream_ts.max() - second_stream_ts.min())\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.plot(first_stream_norm.index, first_stream_norm, \n",
    "                 label=f'Stream {station_id} (normalized)', color='blue', linewidth=1.5)\n",
    "        plt.plot(second_stream_norm.index, second_stream_norm, \n",
    "                 label=f'Stream {second_station_id} (normalized)', color='red', linewidth=1.5)\n",
    "        \n",
    "        plt.title('Comparison of Normalized Runoff Patterns Between Two Streams')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Normalized Flow Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate correlation between the two streams\n",
    "        # Align the two time series first\n",
    "        aligned_df = pd.DataFrame({\n",
    "            f'stream_{station_id}': first_stream_ts,\n",
    "            f'stream_{second_station_id}': second_stream_ts\n",
    "        })\n",
    "        \n",
    "        correlation = aligned_df.corr().iloc[0, 1]\n",
    "        print(f\"Correlation between the two streams: {correlation:.4f}\")\n",
    "        \n",
    "        # This correlation could be useful for multi-stream modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a977e7",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef70e8",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Data Coverage**: We have analyzed runoff data from multiple streams covering approximately two years (April 2021 to April 2023).\n",
    "2. **Data Types**:\n",
    "   - USGS observations (ground truth)\n",
    "   - NWM forecasts at various lead times\n",
    "3. **Temporal Patterns**: The runoff data shows clear seasonal patterns and potentially some recurring events.\n",
    "4. **NWM Performance**: We've quantified how well NWM forecasts match USGS observations.\n",
    "5. **Feature Importance**: Identified key features that correlate strongly with future runoff values.\n",
    "\n",
    "### Next Steps for LSTM Model Development\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Complete the feature engineering process outlined above\n",
    "   - Create a unified dataset with all relevant features\n",
    "   - Handle missing values and normalize/standardize features\n",
    "\n",
    "2. **Sequence Preparation**:\n",
    "   - Create time sequences for LSTM input based on ACF/PACF analysis\n",
    "   - Structure the data in the format [samples, time steps, features]\n",
    "\n",
    "3. **Model Development**:\n",
    "   - Implement the LSTM architecture\n",
    "   - Experiment with different numbers of layers and units\n",
    "   - Add regularization to prevent overfitting\n",
    "\n",
    "4. **Evaluation Framework**:\n",
    "   - Set up proper metrics for runoff forecasting\n",
    "   - Compare with baseline models and NWM forecasts\n",
    "   - Evaluate at different forecast horizons\n",
    "\n",
    "5. **Multi-Stream Approach**:\n",
    "   - Consider using data from multiple streams in a single model\n",
    "   - Explore transfer learning between similar streams\n",
    "\n",
    "6. **Operational Deployment**:\n",
    "   - Plan for model updating as new data becomes available\n",
    "   - Consider ensemble approaches combining LSTM with NWM forecasts\n",
    "\n",
    "This exploratory analysis provides a strong foundation for building an effective LSTM-based runoff forecasting model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
