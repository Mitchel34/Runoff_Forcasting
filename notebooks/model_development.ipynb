{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Development for Runoff Forecasting\n",
    "\n",
    "This notebook explores different deep learning architectures for improving NWM runoff forecasts. We'll compare LSTM, GRU, Transformer, and hybrid models to find the optimal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, Input, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "data_dir = os.path.join('..', 'data', 'processed')\n",
    "models_dir = os.path.join('..', 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    train_val_df = pd.read_csv(os.path.join(data_dir, 'train_validation_data.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'test_data.csv'))\n",
    "    print(f\"Training/validation data shape: {train_val_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data files not found. Please run the preprocessing script first.\")\n",
    "    # Create dummy data for demonstration\n",
    "    print(\"Creating dummy data for demonstration.\")\n",
    "    train_val_df = pd.DataFrame(np.random.random((1000, 10)), columns=[f'feature_{i}' for i in range(9)] + ['target'])\n",
    "    test_df = pd.DataFrame(np.random.random((200, 10)), columns=[f'feature_{i}' for i in range(9)] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature preparation function\n",
    "def prepare_data(df, feature_cols, target_col, sequence_length=24, train=False):\n",
    "    \"\"\"\n",
    "    Prepare sequential data for model training/evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with features and target\n",
    "    feature_cols : list\n",
    "        List of feature column names\n",
    "    target_col : str\n",
    "        Target column name\n",
    "    sequence_length : int\n",
    "        Length of input sequences\n",
    "    train : bool\n",
    "        Whether this is training data (to fit or use existing scalers)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_seq : numpy.ndarray\n",
    "        Sequence feature array\n",
    "    y : numpy.ndarray\n",
    "        Target array\n",
    "    scaler_X, scaler_y : sklearn.preprocessing.StandardScaler\n",
    "        Fitted scalers for features and target\n",
    "    \"\"\"\n",
    "    # Initialize or use existing scalers\n",
    "    if train or not hasattr(prepare_data, 'scaler_X'):\n",
    "        prepare_data.scaler_X = StandardScaler().fit(df[feature_cols])\n",
    "        prepare_data.scaler_y = StandardScaler().fit(df[[target_col]])\n",
    "    \n",
    "    # Scale data\n",
    "    X = prepare_data.scaler_X.transform(df[feature_cols])\n",
    "    y = prepare_data.scaler_y.transform(df[[target_col]]).flatten()\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_seq.append(X[i:i+sequence_length])\n",
    "        y_seq.append(y[i+sequence_length])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq), prepare_data.scaler_X, prepare_data.scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature and target columns\n",
    "# In a real scenario, you would select appropriate columns from your data\n",
    "feature_columns = [col for col in train_val_df.columns if col.startswith('feature_')]\n",
    "target_column = 'target' if 'target' in train_val_df.columns else 'runoff_usgs'\n",
    "\n",
    "# Split training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare sequences\n",
    "sequence_length = 24  # Use 24-hour sequences\n",
    "X_train, y_train, scaler_X, scaler_y = prepare_data(train_df, feature_columns, target_column, sequence_length, train=True)\n",
    "X_val, y_val, _, _ = prepare_data(val_df, feature_columns, target_column, sequence_length)\n",
    "X_test, y_test, _, _ = prepare_data(test_df, feature_columns, target_column, sequence_length)\n",
    "\n",
    "print(f\"Training sequences: {X_train.shape}\")\n",
    "print(f\"Validation sequences: {X_val.shape}\")\n",
    "print(f\"Test sequences: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architectures\n",
    "\n",
    "Let's implement different model architectures for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, output_shape=1):\n",
    "    \"\"\"Create LSTM model\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(output_shape)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_gru_model(input_shape, output_shape=1):\n",
    "    \"\"\"Create GRU model\"\"\"\n",
    "    model = Sequential([\n",
    "        GRU(64, activation='relu', return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        GRU(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(output_shape)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"Create a Transformer encoder block\"\"\"\n",
    "    # Multi-head attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    # Skip connection and layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff_output = Sequential([\n",
    "        Dense(ff_dim, activation='relu'),\n",
    "        Dense(inputs.shape[-1]),\n",
    "        Dropout(dropout)\n",
    "    ])(x)\n",
    "    \n",
    "    # Skip connection and layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def create_transformer_model(input_shape, output_shape=1, head_size=256, num_heads=4, \n",
    "                            ff_dim=512, num_transformer_blocks=4, mlp_units=[128], dropout=0.2):\n",
    "    \"\"\"Create Transformer model\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Final MLP layers\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    \n",
    "    outputs = Dense(output_shape)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_hybrid_model(input_shape, output_shape=1):\n",
    "    \"\"\"Create hybrid CNN-LSTM model\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers for feature extraction\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # LSTM layers for sequential processing\n",
    "    x = LSTM(32, activation='relu', return_sequences=True)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(16, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Dense layers for output\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    outputs = Dense(output_shape)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Compare Models\n",
    "\n",
    "Let's train each model architecture and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, features)\n",
    "\n",
    "# Callbacks for training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model = create_lstm_model(input_shape)\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU model\n",
    "print(\"Training GRU model...\")\n",
    "gru_model = create_gru_model(input_shape)\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer model\n",
    "print(\"Training Transformer model...\")\n",
    "transformer_model = create_transformer_model(input_shape)\n",
    "transformer_history = transformer_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Hybrid model\n",
    "print(\"Training Hybrid CNN-LSTM model...\")\n",
    "hybrid_model = create_hybrid_model(input_shape)\n",
    "hybrid_history = hybrid_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training history\n",
    "def plot_training_history(histories, model_names):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    for history, name in zip(histories, model_names):\n",
    "        # Plot loss\n",
    "        ax1.plot(history.history['loss'], label=f'{name} Training')\n",
    "        ax1.plot(history.history['val_loss'], label=f'{name} Validation')\n",
    "    \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    for history, name in zip(histories, model_names):\n",
    "        # Plot MAE\n",
    "        ax2.plot(history.history['mae'], label=f'{name} Training')\n",
    "        ax2.plot(history.history['val_mae'], label=f'{name} Validation')\n",
    "    \n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_title('Training and Validation MAE')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Plot training histories\n",
    "histories = [lstm_history, gru_history, transformer_history, hybrid_history]\n",
    "model_names = ['LSTM', 'GRU', 'Transformer', 'Hybrid']\n",
    "plot_training_history(histories, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "def evaluate_model(model, X_test, y_test, scaler_y, model_name):\n",
    "    \"\"\"Evaluate model and compute metrics\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    \n",
    "    # Convert to original scale\n",
    "    y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred_orig = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "    \n",
    "    # Calculate hydrological metrics\n",
    "    cc = np.corrcoef(y_test_orig, y_pred_orig)[0, 1]\n",
    "    pbias = 100 * np.sum(y_pred_orig - y_test_orig) / np.sum(y_test_orig) if np.sum(y_test_orig) != 0 else np.nan\n",
    "    nse = 1 - (np.sum((y_test_orig - y_pred_orig) ** 2) / np.sum((y_test_orig - np.mean(y_test_orig)) ** 2))\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'CC': cc,\n",
    "        'PBIAS': pbias,\n",
    "        'NSE': nse\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "models = [lstm_model, gru_model, transformer_model, hybrid_model]\n",
    "model_names = ['LSTM', 'GRU', 'Transformer', 'Hybrid']\n",
    "\n",
    "results = []\n",
    "for model, name in zip(models, model_names):\n",
    "    metrics = evaluate_model(model, X_test, y_test, scaler_y, name)\n",
    "    results.append(metrics)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of models\n",
    "metrics_to_plot = ['RMSE', 'MAE', 'CC', 'NSE']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[i]\n",
    "    ax.bar(results_df['Model'], results_df[metric])\n",
    "    ax.set_title(f'Comparison of {metric}')\n",
    "    ax.set_ylabel(metric)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(results_df[metric]):\n",
    "        ax.text(j, v, f\"{v:.4f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Best Model and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model based on NSE\n",
    "best_model_idx = results_df['NSE'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "best_model = models[best_model_idx]\n",
    "\n",
    "print(f\"Best model is {best_model_name} with NSE = {results_df.loc[best_model_idx, 'NSE']:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "model_path = os.path.join(models_dir, 'nwm_dl_model.keras')\n",
    "best_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions from Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with best model\n",
    "y_pred = best_model.predict(X_test).flatten()\n",
    "\n",
    "# Convert to original scale\n",
    "y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "y_pred_orig = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_orig, label='Actual')\n",
    "plt.plot(y_pred_orig, label='Predicted')\n",
    "plt.title(f'{best_model_name} Model: Predictions vs Actual')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Runoff')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test_orig, y_pred_orig, alpha=0.5)\n",
    "max_val = max(np.max(y_test_orig), np.max(y_pred_orig)) * 1.1\n",
    "plt.plot([0, max_val], [0, max_val], 'k--')\n",
    "plt.xlim([0, max_val])\n",
    "plt.ylim([0, max_val])\n",
    "plt.title(f'{best_model_name} Model: Predicted vs Actual')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "Based on our experiments, we can draw the following conclusions:\n",
    "\n",
    "1. The [best_model_name] model performed best, achieving the highest NSE value of [NSE_value].\n",
    "2. [Add other observations about performance differences].\n",
    "3. The model successfully [mention any specific improvements over baseline NWM forecasts].\n",
    "4. Areas for future improvement include [your suggestions].\n",
    "\n",
    "These results demonstrate that deep learning approaches can effectively improve NWM runoff forecasts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
