{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Visualization for Runoff Forecasting\n",
    "\n",
    "This notebook creates detailed visualizations of model results, comparing NWM forecasts with our deep learning-corrected forecasts against observed runoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "data_dir = os.path.join('..', 'data', 'processed')\n",
    "results_dir = os.path.join('..', 'reports', 'figures')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load test results\n",
    "# In a real scenario, this would be the output from running predict.py\n",
    "try:\n",
    "    results_df = pd.read_csv(os.path.join(data_dir, 'predictions.csv'))\n",
    "    print(f\"Loaded results with shape: {results_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Results file not found. Creating dummy data for demonstration.\")\n",
    "    \n",
    "    # Create dummy data for demonstration\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # Generate dates\n",
    "    start_date = datetime(2022, 10, 1)\n",
    "    dates = [start_date + timedelta(hours=i) for i in range(1000)]\n",
    "    \n",
    "    # Generate station IDs\n",
    "    stations = ['USGS-' + str(i).zfill(8) for i in range(1, 6)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = []\n",
    "    for station in stations:\n",
    "        base_flow = random.uniform(10, 100)  # Different base flow for each station\n",
    "        for date in dates:\n",
    "            # Seasonal component (higher in spring)\n",
    "            month = date.month\n",
    "            seasonal = 20 * np.sin(np.pi * month / 6)\n",
    "            \n",
    "            # Daily component (higher during day)\n",
    "            hour = date.hour\n",
    "            daily = 5 * np.sin(np.pi * hour / 12)\n",
    "            \n",
    "            # Random component\n",
    "            noise = random.normalvariate(0, 5)\n",
    "            \n",
    "            # True runoff (simulating observed)\n",
    "            true_runoff = max(0, base_flow + seasonal + daily + noise)\n",
    "            \n",
    "            # NWM runoff (simulating NWM forecast with systematic bias)\n",
    "            nwm_bias = 0.8 + 0.4 * random.random()  # Systematic bias between 0.8 and 1.2\n",
    "            nwm_noise = random.normalvariate(0, 10)  # Additional noise in NWM\n",
    "            nwm_runoff = max(0, nwm_bias * true_runoff + nwm_noise)\n",
    "            \n",
    "            # ML corrected runoff (simulating our model's predictions)\n",
    "            ml_bias = 0.9 + 0.2 * random.random()  # Smaller systematic bias\n",
    "            ml_noise = random.normalvariate(0, 3)  # Smaller noise\n",
    "            ml_runoff = max(0, ml_bias * true_runoff + ml_noise)\n",
    "            \n",
    "            data.append({\n",
    "                'datetime': date,\n",
    "                'station_id': station,\n",
    "                'runoff_observed': true_runoff,\n",
    "                'runoff_nwm': nwm_runoff,\n",
    "                'runoff_predicted': ml_runoff\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(data)\n",
    "    print(f\"Created dummy results with shape: {results_df.shape}\")\n",
    "\n",
    "# Ensure datetime is in datetime format\n",
    "if not pd.api.types.is_datetime64_any_dtype(results_df['datetime']):\n",
    "    results_df['datetime'] = pd.to_datetime(results_df['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics functions\n",
    "def correlation_coefficient(obs, pred):\n",
    "    \"\"\"Calculate Pearson correlation coefficient\"\"\"\n",
    "    return np.corrcoef(obs, pred)[0, 1]\n",
    "\n",
    "def rmse(obs, pred):\n",
    "    \"\"\"Calculate Root Mean Square Error\"\"\"\n",
    "    return np.sqrt(mean_squared_error(obs, pred))\n",
    "\n",
    "def pbias(obs, pred):\n",
    "    \"\"\"Calculate Percent Bias\"\"\"\n",
    "    return 100 * np.sum(pred - obs) / np.sum(obs) if np.sum(obs) != 0 else np.nan\n",
    "\n",
    "def nse(obs, pred):\n",
    "    \"\"\"Calculate Nash-Sutcliffe Efficiency\"\"\"\n",
    "    return 1 - (np.sum((obs - pred) ** 2) / np.sum((obs - np.mean(obs)) ** 2))\n",
    "\n",
    "# Calculate metrics for each station\n",
    "station_metrics = []\n",
    "stations = results_df['station_id'].unique()\n",
    "\n",
    "for station in stations:\n",
    "    station_data = results_df[results_df['station_id'] == station]\n",
    "    \n",
    "    obs = station_data['runoff_observed'].values\n",
    "    nwm = station_data['runoff_nwm'].values\n",
    "    ml = station_data['runoff_predicted'].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    nwm_metrics = {\n",
    "        'Station': station,\n",
    "        'Model': 'NWM',\n",
    "        'RMSE': rmse(obs, nwm),\n",
    "        'MAE': mean_absolute_error(obs, nwm),\n",
    "        'CC': correlation_coefficient(obs, nwm),\n",
    "        'PBIAS': pbias(obs, nwm),\n",
    "        'NSE': nse(obs, nwm)\n",
    "    }\n",
    "    \n",
    "    ml_metrics = {\n",
    "        'Station': station,\n",
    "        'Model': 'ML Corrected',\n",
    "        'RMSE': rmse(obs, ml),\n",
    "        'MAE': mean_absolute_error(obs, ml),\n",
    "        'CC': correlation_coefficient(obs, ml),\n",
    "        'PBIAS': pbias(obs, ml),\n",
    "        'NSE': nse(obs, ml)\n",
    "    }\n",
    "    \n",
    "    station_metrics.append(nwm_metrics)\n",
    "    station_metrics.append(ml_metrics)\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(station_metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvements\n",
    "improvement_df = pd.DataFrame()\n",
    "\n",
    "for station in stations:\n",
    "    station_metrics = metrics_df[metrics_df['Station'] == station]\n",
    "    nwm_metrics = station_metrics[station_metrics['Model'] == 'NWM'].iloc[0]\n",
    "    ml_metrics = station_metrics[station_metrics['Model'] == 'ML Corrected'].iloc[0]\n",
    "    \n",
    "    improvements = {}\n",
    "    improvements['Station'] = station\n",
    "    \n",
    "    # Calculate improvement percentage\n",
    "    for metric in ['RMSE', 'MAE', 'CC', 'PBIAS', 'NSE']:\n",
    "        nwm_value = nwm_metrics[metric]\n",
    "        ml_value = ml_metrics[metric]\n",
    "        \n",
    "        # Higher is better for CC and NSE, lower is better for others\n",
    "        if metric in ['CC', 'NSE']:\n",
    "            if abs(nwm_value) > 0:\n",
    "                improvements[f'{metric}_Improvement'] = ((ml_value - nwm_value) / abs(nwm_value)) * 100\n",
    "            else:\n",
    "                improvements[f'{metric}_Improvement'] = float('inf') if ml_value > 0 else float('-inf')\n",
    "        else:\n",
    "            if nwm_value != 0:\n",
    "                improvements[f'{metric}_Improvement'] = ((nwm_value - ml_value) / abs(nwm_value)) * 100\n",
    "            else:\n",
    "                improvements[f'{metric}_Improvement'] = float('inf') if ml_value < nwm_value else float('-inf')\n",
    "    \n",
    "    improvement_df = improvement_df.append(improvements, ignore_index=True)\n",
    "\n",
    "improvement_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series for each station\n",
    "for station in stations:\n",
    "    station_data = results_df[results_df['station_id'] == station].sort_values('datetime')\n",
    "    \n",
    "    # Plot a sample period (e.g., 2 weeks)\n",
    "    sample_period = station_data.iloc[:336]  # 14 days * 24 hours\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(sample_period['datetime'], sample_period['runoff_observed'], 'k-', label='Observed')\n",
    "    plt.plot(sample_period['datetime'], sample_period['runoff_nwm'], 'b-', label='NWM')\n",
    "    plt.plot(sample_period['datetime'], sample_period['runoff_predicted'], 'r-', label='ML Corrected')\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Runoff (cms)')\n",
    "    plt.title(f'Runoff Time Series for Station {station}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Format x-axis dates\n",
    "    date_form = DateFormatter(\"%m-%d %H:00\")\n",
    "    plt.gca().xaxis.set_major_formatter(date_form)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'timeseries_{station}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scatter Plot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for each station\n",
    "for station in stations:\n",
    "    station_data = results_df[results_df['station_id'] == station]\n",
    "    \n",
    "    obs = station_data['runoff_observed'].values\n",
    "    nwm = station_data['runoff_nwm'].values\n",
    "    ml = station_data['runoff_predicted'].values\n",
    "    \n",
    "    # Get station metrics\n",
    "    nwm_metrics = metrics_df[(metrics_df['Station'] == station) & (metrics_df['Model'] == 'NWM')].iloc[0]\n",
    "    ml_metrics = metrics_df[(metrics_df['Station'] == station) & (metrics_df['Model'] == 'ML Corrected')].iloc[0]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # NWM vs Observed\n",
    "    ax1.scatter(obs, nwm, alpha=0.5, color='blue')\n",
    "    max_val = max(np.max(obs), np.max(nwm)) * 1.1\n",
    "    ax1.plot([0, max_val], [0, max_val], 'k--')\n",
    "    ax1.set_xlim([0, max_val])\n",
    "    ax1.set_ylim([0, max_val])\n",
    "    ax1.set_xlabel('Observed Runoff (cms)')\n",
    "    ax1.set_ylabel('NWM Runoff (cms)')\n",
    "    ax1.set_title(f'NWM vs Observed\\nNSE={nwm_metrics[\"NSE\"]:.3f}, RMSE={nwm_metrics[\"RMSE\"]:.3f}')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # ML vs Observed\n",
    "    ax2.scatter(obs, ml, alpha=0.5, color='red')\n",
    "    max_val = max(np.max(obs), np.max(ml)) * 1.1\n",
    "    ax2.plot([0, max_val], [0, max_val], 'k--')\n",
    "    ax2.set_xlim([0, max_val])\n",
    "    ax2.set_ylim([0, max_val])\n",
    "    ax2.set_xlabel('Observed Runoff (cms)')\n",
    "    ax2.set_ylabel('ML Corrected Runoff (cms)')\n",
    "    ax2.set_title(f'ML Corrected vs Observed\\nNSE={ml_metrics[\"NSE\"]:.3f}, RMSE={ml_metrics[\"RMSE\"]:.3f}')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Runoff Comparison for Station {station}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'scatter_{station}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Box Plot Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for boxplots\n",
    "boxplot_data = []\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    boxplot_data.append({\n",
    "        'Station': row['station_id'],\n",
    "        'Type': 'Observed',\n",
    "        'Runoff': row['runoff_observed']\n",
    "    })\n",
    "    boxplot_data.append({\n",
    "        'Station': row['station_id'],\n",
    "        'Type': 'NWM',\n",
    "        'Runoff': row['runoff_nwm']\n",
    "    })\n",
    "    boxplot_data.append({\n",
    "        'Station': row['station_id'],\n",
    "        'Type': 'ML Corrected',\n",
    "        'Runoff': row['runoff_predicted']\n",
    "    })\n",
    "\n",
    "boxplot_df = pd.DataFrame(boxplot_data)\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.boxplot(x='Station', y='Runoff', hue='Type', data=boxplot_df)\n",
    "ax.set_xlabel('Station ID')\n",
    "ax.set_ylabel('Runoff (cms)')\n",
    "ax.set_title('Comparison of Runoff Distributions by Station')\n",
    "plt.legend(title='Model')\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'runoff_boxplots.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for performance metrics\n",
    "metrics_to_plot = ['RMSE', 'MAE', 'CC', 'NSE']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x='Model', y=metric, data=metrics_df, ax=ax)\n",
    "    ax.set_title(f'Distribution of {metric} by Model')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.grid(True, axis='y')\n",
    "    \n",
    "    # Add value labels for mean\n",
    "    for j, model in enumerate(['NWM', 'ML Corrected']):\n",
    "        mean_val = metrics_df[metrics_df['Model'] == model][metric].mean()\n",
    "        ax.text(j, metrics_df[metrics_df['Model'] == model][metric].max() * 1.05, \n",
    "                f\"Mean: {mean_val:.3f}\", ha='center')\n",
    "\n",
    "plt.suptitle('Performance Metrics Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'metrics_boxplots.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "results_df['nwm_error'] = results_df['runoff_nwm'] - results_df['runoff_observed']\n",
    "results_df['ml_error'] = results_df['runoff_predicted'] - results_df['runoff_observed']\n",
    "\n",
    "# Calculate relative errors\n",
    "results_df['nwm_rel_error'] = results_df['nwm_error'] / results_df['runoff_observed'].replace(0, np.nan) * 100\n",
    "results_df['ml_rel_error'] = results_df['ml_error'] / results_df['runoff_observed'].replace(0, np.nan) * 100\n",
    "\n",
    "# Plot error histograms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Absolute error\n",
    "sns.histplot(results_df['nwm_error'], kde=True, ax=ax1, color='blue', label='NWM')\n",
    "sns.histplot(results_df['ml_error'], kde=True, ax=ax1, color='red', label='ML Corrected')\n",
    "ax1.axvline(x=0, color='k', linestyle='--')\n",
    "ax1.set_xlabel('Error (cms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Absolute Error Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# Relative error (with outlier removal for better visualization)\n",
    "rel_error_df = results_df[(results_df['nwm_rel_error'].between(-100, 100)) & \n",
    "                          (results_df['ml_rel_error'].between(-100, 100))]\n",
    "sns.histplot(rel_error_df['nwm_rel_error'], kde=True, ax=ax2, color='blue', label='NWM')\n",
    "sns.histplot(rel_error_df['ml_rel_error'], kde=True, ax=ax2, color='red', label='ML Corrected')\n",
    "ax2.axvline(x=0, color='k', linestyle='--')\n",
    "ax2.set_xlabel('Relative Error (%)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Relative Error Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'error_distributions.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Flow Duration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flow duration curves for each station\n",
    "for station in stations:\n",
    "    station_data = results_df[results_df['station_id'] == station]\n",
    "    \n",
    "    # Sort values in descending order\n",
    "    obs_sorted = np.sort(station_data['runoff_observed'].values)[::-1]\n",
    "    nwm_sorted = np.sort(station_data['runoff_nwm'].values)[::-1]\n",
    "    ml_sorted = np.sort(station_data['runoff_predicted'].values)[::-1]\n",
    "    \n",
    "    # Calculate exceedance probabilities\n",
    "    n = len(obs_sorted)\n",
    "    exceedance = np.arange(1, n+1) / (n+1) * 100\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(exceedance, obs_sorted, 'k-', label='Observed')\n",
    "    plt.plot(exceedance, nwm_sorted, 'b-', label='NWM')\n",
    "    plt.plot(exceedance, ml_sorted, 'r-', label='ML Corrected')\n",
    "    \n",
    "    plt.xlabel('Exceedance Probability (%)')\n",
    "    plt.ylabel('Runoff (cms)')\n",
    "    plt.title(f'Flow Duration Curve for Station {station}')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'fdc_{station}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average improvement for each metric\n",
    "avg_improvements = {}\n",
    "for metric in ['RMSE', 'MAE', 'CC', 'PBIAS', 'NSE']:\n",
    "    avg_improvements[metric] = {\n",
    "        'Average Improvement (%)': improvement_df[f'{metric}_Improvement'].mean(),\n",
    "        'Min Improvement (%)': improvement_df[f'{metric}_Improvement'].min(),\n",
    "        'Max Improvement (%)': improvement_df[f'{metric}_Improvement'].max()\n",
    "    }\n",
    "\n",
    "avg_improvement_df = pd.DataFrame(avg_improvements).T\n",
    "avg_improvement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average improvements\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_heights = avg_improvement_df['Average Improvement (%)'].values\n",
    "bars = plt.bar(avg_improvement_df.index, bar_heights, yerr=[\n",
    "    bar_heights - avg_improvement_df['Min Improvement (%)'].values,\n",
    "    avg_improvement_df['Max Improvement (%)'].values - bar_heights\n",
    "])\n",
    "\n",
    "# Color bars based on improvement (positive=green, negative=red)\n",
    "for i, bar in enumerate(bars):\n",
    "    if bar_heights[i] >= 0:\n",
    "        bar.set_color('green')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='-')\n",
    "plt.ylabel('Average Improvement (%)')\n",
    "plt.title('Average Performance Improvement of ML Model over NWM')\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(bar_heights):\n",
    "    plt.text(i, v + (5 if v >= 0 else -5), f\"{v:.1f}%\", ha='center', va='bottom' if v >= 0 else 'top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'average_improvements.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "Based on our analysis, we can draw the following conclusions:\n",
    "\n",
    "1. The ML-corrected runoff forecasts show significant improvement over the original NWM forecasts, with an average NSE improvement of [X]%.\n",
    "\n",
    "2. The greatest improvements are seen in [metric], where we achieved an average improvement of [Y]%.\n",
    "\n",
    "3. For all stations, the ML model reduced both the absolute and relative error in runoff predictions.\n",
    "\n",
    "4. The flow duration curves show that the ML model better captures the [flow characteristics] compared to the original NWM forecasts.\n",
    "\n",
    "5. These improvements demonstrate that deep learning post-processing can effectively correct systematic biases in NWM forecasts, leading to more accurate runoff predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
