{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Runoff Forecasting\n",
    "\n",
    "This notebook explores the National Water Model (NWM) forecasts and USGS observational data to understand patterns, biases, and potential for improvement using deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "# Configure notebook display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = os.path.join('..', 'data', 'raw')\n",
    "nwm_file = os.path.join(data_dir, 'nwm_forecasts.csv')\n",
    "usgs_file = os.path.join(data_dir, 'usgs_observations.csv')\n",
    "\n",
    "# Load NWM forecast data\n",
    "try:\n",
    "    nwm_df = pd.read_csv(nwm_file, parse_dates=['datetime'])\n",
    "    print(f\"NWM data loaded with shape: {nwm_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"NWM file not found: {nwm_file}\")\n",
    "    nwm_df = pd.DataFrame()\n",
    "    \n",
    "# Load USGS observation data\n",
    "try:\n",
    "    usgs_df = pd.read_csv(usgs_file, parse_dates=['datetime'])\n",
    "    print(f\"USGS data loaded with shape: {usgs_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"USGS file not found: {usgs_file}\")\n",
    "    usgs_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is available, examine the structure\n",
    "if not nwm_df.empty:\n",
    "    print(\"\\nNWM Data Sample:\")\n",
    "    display(nwm_df.head())\n",
    "    print(\"\\nNWM Data Info:\")\n",
    "    display(nwm_df.info())\n",
    "    print(\"\\nNWM Data Description:\")\n",
    "    display(nwm_df.describe())\n",
    "\n",
    "if not usgs_df.empty:\n",
    "    print(\"\\nUSGS Data Sample:\")\n",
    "    display(usgs_df.head())\n",
    "    print(\"\\nUSGS Data Info:\")\n",
    "    display(usgs_df.info())\n",
    "    print(\"\\nUSGS Data Description:\")\n",
    "    display(usgs_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in NWM data\n",
    "if not nwm_df.empty:\n",
    "    print(\"Missing values in NWM data:\")\n",
    "    display(nwm_df.isnull().sum())\n",
    "    print(f\"Total missing values: {nwm_df.isnull().sum().sum()}\")\n",
    "    \n",
    "# Check for missing values in USGS data\n",
    "if not usgs_df.empty:\n",
    "    print(\"\\nMissing values in USGS data:\")\n",
    "    display(usgs_df.isnull().sum())\n",
    "    print(f\"Total missing values: {usgs_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Temporal Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the time range for each dataset\n",
    "if not nwm_df.empty:\n",
    "    print(\"NWM data temporal coverage:\")\n",
    "    print(f\"Start: {nwm_df['datetime'].min()}\")\n",
    "    print(f\"End: {nwm_df['datetime'].max()}\")\n",
    "    print(f\"Duration: {nwm_df['datetime'].max() - nwm_df['datetime'].min()}\")\n",
    "    \n",
    "if not usgs_df.empty:\n",
    "    print(\"\\nUSGS data temporal coverage:\")\n",
    "    print(f\"Start: {usgs_df['datetime'].min()}\")\n",
    "    print(f\"End: {usgs_df['datetime'].max()}\")\n",
    "    print(f\"Duration: {usgs_df['datetime'].max() - usgs_df['datetime'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Station Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of stations in each dataset\n",
    "if not nwm_df.empty and 'station_id' in nwm_df.columns:\n",
    "    nwm_stations = nwm_df['station_id'].unique()\n",
    "    print(f\"Number of stations in NWM data: {len(nwm_stations)}\")\n",
    "    \n",
    "if not usgs_df.empty and 'station_id' in usgs_df.columns:\n",
    "    usgs_stations = usgs_df['station_id'].unique()\n",
    "    print(f\"Number of stations in USGS data: {len(usgs_stations)}\")\n",
    "    \n",
    "# Check for overlap between datasets\n",
    "if not nwm_df.empty and not usgs_df.empty and 'station_id' in nwm_df.columns and 'station_id' in usgs_df.columns:\n",
    "    common_stations = set(nwm_stations).intersection(set(usgs_stations))\n",
    "    print(f\"Number of common stations: {len(common_stations)}\")\n",
    "    print(f\"Percentage of NWM stations with USGS data: {len(common_stations)/len(nwm_stations)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Distribution of Runoff Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize runoff distributions if data is available\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# NWM Runoff Distribution\n",
    "if not nwm_df.empty and 'runoff_nwm' in nwm_df.columns:\n",
    "    sns.histplot(nwm_df['runoff_nwm'], bins=50, kde=True, ax=axes[0])\n",
    "    axes[0].set_title('NWM Runoff Distribution')\n",
    "    axes[0].set_xlabel('Runoff (cms)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log scale for better visualization of extreme values\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "# USGS Runoff Distribution\n",
    "if not usgs_df.empty and 'runoff_usgs' in usgs_df.columns:\n",
    "    sns.histplot(usgs_df['runoff_usgs'], bins=50, kde=True, ax=axes[1])\n",
    "    axes[1].set_title('USGS Runoff Distribution')\n",
    "    axes[1].set_xlabel('Runoff (cms)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log scale for better visualization of extreme values\n",
    "    axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge Datasets for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge NWM and USGS datasets if both are available\n",
    "if not nwm_df.empty and not usgs_df.empty:\n",
    "    # Align on datetime and station_id\n",
    "    merged_df = pd.merge(\n",
    "        nwm_df,\n",
    "        usgs_df,\n",
    "        on=['datetime', 'station_id'],\n",
    "        how='inner',\n",
    "        suffixes=('_nwm', '_usgs')\n",
    "    )\n",
    "    \n",
    "    print(f\"Merged data shape: {merged_df.shape}\")\n",
    "    display(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze NWM Forecast Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias if merged data is available\n",
    "if 'merged_df' in locals() and 'runoff_nwm' in merged_df.columns and 'runoff_usgs' in merged_df.columns:\n",
    "    # Calculate absolute and relative errors\n",
    "    merged_df['error'] = merged_df['runoff_nwm'] - merged_df['runoff_usgs']\n",
    "    merged_df['rel_error'] = merged_df['error'] / merged_df['runoff_usgs'] * 100\n",
    "    \n",
    "    # Summary statistics of error\n",
    "    print(\"Error summary statistics:\")\n",
    "    display(merged_df[['error', 'rel_error']].describe())\n",
    "    \n",
    "    # Visualize error distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Absolute error\n",
    "    sns.histplot(merged_df['error'], bins=50, kde=True, ax=ax1)\n",
    "    ax1.axvline(x=0, color='r', linestyle='--')\n",
    "    ax1.set_title('NWM Absolute Error Distribution')\n",
    "    ax1.set_xlabel('Error (cms)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    \n",
    "    # Relative error (with outlier removal for better visualization)\n",
    "    rel_error_filtered = merged_df['rel_error'].clip(-200, 200)  # Clip extreme values\n",
    "    sns.histplot(rel_error_filtered, bins=50, kde=True, ax=ax2)\n",
    "    ax2.axvline(x=0, color='r', linestyle='--')\n",
    "    ax2.set_title('NWM Relative Error Distribution (clipped)')\n",
    "    ax2.set_xlabel('Relative Error (%)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Temporal Patterns in Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns in NWM error\n",
    "if 'merged_df' in locals() and 'error' in merged_df.columns:\n",
    "    # Extract time components\n",
    "    merged_df['hour'] = merged_df['datetime'].dt.hour\n",
    "    merged_df['month'] = merged_df['datetime'].dt.month\n",
    "    merged_df['season'] = pd.cut(\n",
    "        merged_df['datetime'].dt.month, \n",
    "        bins=[0, 3, 6, 9, 12], \n",
    "        labels=['Winter', 'Spring', 'Summer', 'Fall']\n",
    "    )\n",
    "    \n",
    "    # Error by hour of day\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    hourly_error = merged_df.groupby('hour')['error'].mean()\n",
    "    hourly_error.plot(kind='line', marker='o', ax=ax1)\n",
    "    ax1.set_title('Mean NWM Error by Hour of Day')\n",
    "    ax1.set_xlabel('Hour')\n",
    "    ax1.set_ylabel('Mean Error (cms)')\n",
    "    ax1.set_xticks(range(0, 24, 2))\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # Error by month\n",
    "    monthly_error = merged_df.groupby('month')['error'].mean()\n",
    "    monthly_error.plot(kind='line', marker='o', ax=ax2)\n",
    "    ax2.set_title('Mean NWM Error by Month')\n",
    "    ax2.set_xlabel('Month')\n",
    "    ax2.set_ylabel('Mean Error (cms)')\n",
    "    ax2.set_xticks(range(1, 13))\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Error by season (box plot)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='season', y='error', data=merged_df)\n",
    "    plt.title('NWM Error Distribution by Season')\n",
    "    plt.xlabel('Season')\n",
    "    plt.ylabel('Error (cms)')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Station-Specific Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze patterns by station\n",
    "if 'merged_df' in locals() and 'station_id' in merged_df.columns:\n",
    "    # Calculate station-wise error statistics\n",
    "    station_stats = merged_df.groupby('station_id').agg({\n",
    "        'error': ['mean', 'std', 'median'],\n",
    "        'rel_error': ['mean', 'std', 'median'],\n",
    "        'runoff_usgs': ['mean', 'min', 'max', 'count']\n",
    "    })\n",
    "    \n",
    "    # Flatten the column hierarchy\n",
    "    station_stats.columns = ['_'.join(col).strip() for col in station_stats.columns.values]\n",
    "    \n",
    "    # Sort by absolute mean error\n",
    "    station_stats = station_stats.sort_values(by='error_mean', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Station-wise error statistics (top 10 by absolute error):\")\n",
    "    display(station_stats.head(10))\n",
    "    \n",
    "    # Visualize station-specific error patterns\n",
    "    # Focus on stations with reasonable data (e.g., at least 100 records)\n",
    "    stations_to_plot = station_stats[station_stats['runoff_usgs_count'] >= 100].index[:5]\n",
    "    \n",
    "    for station in stations_to_plot:\n",
    "        station_data = merged_df[merged_df['station_id'] == station]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Scatter plot of predicted vs observed\n",
    "        ax1.scatter(station_data['runoff_usgs'], station_data['runoff_nwm'], alpha=0.5)\n",
    "        max_val = max(station_data['runoff_usgs'].max(), station_data['runoff_nwm'].max()) * 1.1\n",
    "        ax1.plot([0, max_val], [0, max_val], 'r--')\n",
    "        ax1.set_xlabel('Observed Runoff (cms)')\n",
    "        ax1.set_ylabel('NWM Predicted Runoff (cms)')\n",
    "        ax1.set_title(f'Station {station}: NWM vs Observed')\n",
    "        \n",
    "        # Time series of a sample period (e.g., 30 days)\n",
    "        sample_period = station_data.sort_values('datetime').iloc[:720]  # ~30 days of hourly data\n",
    "        ax2.plot(sample_period['datetime'], sample_period['runoff_usgs'], 'b-', label='Observed')\n",
    "        ax2.plot(sample_period['datetime'], sample_period['runoff_nwm'], 'r-', label='NWM')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel('Runoff (cms)')\n",
    "        ax2.set_title(f'Station {station}: Sample Time Series')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyze Relationship Between Error and Flow Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how error relates to flow magnitude\n",
    "if 'merged_df' in locals() and 'runoff_usgs' in merged_df.columns and 'error' in merged_df.columns:\n",
    "    # Create flow magnitude bins\n",
    "    merged_df['flow_bin'] = pd.qcut(merged_df['runoff_usgs'], q=10, duplicates='drop')\n",
    "    \n",
    "    # Calculate error statistics by flow bin\n",
    "    flow_bin_stats = merged_df.groupby('flow_bin').agg({\n",
    "        'error': ['mean', 'std', 'median'],\n",
    "        'rel_error': ['mean', 'std', 'median'],\n",
    "        'runoff_usgs': ['mean', 'count']\n",
    "    })\n",
    "    \n",
    "    # Flatten the column hierarchy\n",
    "    flow_bin_stats.columns = ['_'.join(col).strip() for col in flow_bin_stats.columns.values]\n",
    "    \n",
    "    print(\"Error statistics by flow magnitude bin:\")\n",
    "    display(flow_bin_stats)\n",
    "    \n",
    "    # Visualize relationship between error and flow magnitude\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Scatter plot with flow magnitude\n",
    "    ax1.scatter(merged_df['runoff_usgs'], merged_df['error'], alpha=0.1)\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    ax1.set_xlabel('Observed Runoff (cms)')\n",
    "    ax1.set_ylabel('Error (cms)')\n",
    "    ax1.set_title('NWM Error vs Flow Magnitude')\n",
    "    \n",
    "    # Box plot of relative error by flow bin\n",
    "    sns.boxplot(x='flow_bin', y='rel_error', data=merged_df, ax=ax2)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    ax2.set_xlabel('Flow Magnitude Bin')\n",
    "    ax2.set_ylabel('Relative Error (%)')\n",
    "    ax2.set_title('Relative Error by Flow Magnitude')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=90)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Insights for Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "\n",
    "1. **Data Coverage**: [Summary of temporal and spatial coverage]\n",
    "\n",
    "2. **Bias Patterns**: [Summary of observed bias patterns]\n",
    "   - Temporal patterns: [hourly, seasonal variations]\n",
    "   - Station-specific patterns: [variations across stations]\n",
    "   - Flow magnitude relationships: [how error varies with flow magnitude]\n",
    "\n",
    "3. **Missing Data**: [Summary of missing data issues]\n",
    "\n",
    "### Implications for Model Development:\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Include temporal features (hour, day, month, season)\n",
    "   - Include station-specific features or embeddings\n",
    "   - Consider flow magnitude as a feature\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Sequential models (LSTM/GRU) to capture temporal dependencies\n",
    "   - Consider station-specific models for locations with unique patterns\n",
    "   - Implement bias correction techniques specifically for high-flow events\n",
    "\n",
    "3. **Data Preparation**:\n",
    "   - Handle missing values appropriately\n",
    "   - Consider data normalization strategies\n",
    "   - Ensure adequate representation of both high and low flow events in training data\n",
    "\n",
    "4. **Evaluation Metrics**:\n",
    "   - Focus on relevant hydrological metrics (RMSE, NSE, PBIAS)\n",
    "   - Consider performance across different flow regimes\n",
    "   - Evaluate performance by season and station"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
