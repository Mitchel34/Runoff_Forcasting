# Plot Selection Guide for Runoff Forecasting Presentation

This document maps the plots requested in the presentation slides to specific files generated by the `evaluate.py` script, ensuring you use the most relevant and compelling visualizations.

## Station-Specific Results

### Station 21609641 (California)

**Slide 9: Key Results - Station 21609641**

1. **Runoff Distribution Boxplot**
   - File: `results/plots/21609641_lstm_runoff_boxplot.png` or `results/plots/21609641_transformer_runoff_boxplot.png`
   - Description: Blue boxes show observed USGS flow (with long whiskers indicating high variability), orange boxes show NWM forecasts, green boxes show corrected forecasts
   - Discussion: Point out how the corrected forecasts (green) better match the observed distribution than the original NWM forecasts (orange), particularly in matching the median values across lead times

2. **Performance Metrics Line Plot**
   - File: `results/plots/21609641_lstm_NSE_lineplot.png` or `results/plots/21609641_lstm_RMSE_lineplot.png` 
   - Description: Shows how model performance changes across lead times (1-18h)
   - Discussion: Highlight how the corrected forecasts maintain better skill (higher NSE values or lower RMSE) than the original NWM forecasts, especially at longer lead times (12-18h)

### Station 20380357 (Colorado)

**Slide 10: Key Results - Station 20380357**

1. **Runoff Distribution Boxplot**
   - File: `results/plots/20380357_lstm_runoff_boxplot.png` or `results/plots/20380357_transformer_runoff_boxplot.png`
   - Description: Shows the dramatic contrast between observed flows (very low, near 0-2 cfs with short whiskers) and NWM forecasts (much higher, up to 80+ cfs)
   - Discussion: Point out the extremely short whiskers in the observed data boxplot (blue), indicating very consistent low flows, versus the huge overestimation by NWM (orange). Note how the corrected forecasts (green) reduce but don't completely fix this overestimation

2. **Monthly Metrics Distribution Boxplot**
   - File: `results/plots/20380357_lstm_PBIAS_distribution_boxplot.png` or `results/plots/20380357_transformer_RMSE_distribution_boxplot.png`
   - Description: Shows the distribution of metrics across different months in the test period
   - Discussion: These plots demonstrate that the NWM consistently overestimated flow across different seasons/months, and while our models reduced this bias, they couldn't fully correct such severe baseline errors

## Cross-Station Comparisons

**Slide 11: Discussion**

1. **Metrics Improvement Comparison**
   - Suggestion: Create a simple bar chart comparing percent improvement in key metrics (RMSE, PBIAS, NSE) between stations
   - Discussion: This visualization clearly shows the stark difference in correction success between stations - highlighting how the baseline NWM quality strongly influences correction potential

## Before & After Correction Example

**Slide 13: Conclusion**

1. **Time Series "Before & After" for High-Flow Event**
   - Suggestion: Extract a ~5-day window from the test period showing a high-flow event for Station 21609641
   - Include: USGS observed flow (blue line), NWM forecast (orange line), and corrected forecast (green line) for a specific lead time (e.g., 6h)
   - Discussion: This compelling example illustrates real-world impact - how correcting NWM errors could improve flood forecasting during critical events

## Additional Plot Options:

1. **Raw Data Visualization (Slide 4)**:
   - Create a simple time series plot showing 1-2 weeks of USGS observations vs NWM forecasts (lead time 1h) for both stations
   - Use matplotlib directly with the raw data to highlight the baseline differences

2. **Preprocessing Visualization (Slide 5)**:
   - Create a diagram showing the sequence creation process with sample data
   - This isn't generated by evaluate.py but would help explain your methodology

3. **Model Architecture Diagrams (Slide 6)**:
   - Use visualization tools like netron or create custom diagrams showing LSTM and Transformer structures

4. **Learning Curves (Slide 7)**:
   - Plot training history from model.fit() outputs (loss and val_loss vs epochs)

## Notes:

1. When using boxplots from evaluate.py, remember:
   - Blue = Observed (USGS) - ground truth
   - Orange = Original NWM forecasts
   - Green = Model-corrected forecasts

2. For line plots of metrics, remember:
   - Lower RMSE and PBIAS values indicate better performance
   - Higher CC and NSE values indicate better performance
   
3. For both stations, select either LSTM or Transformer model results based on which one provides clearer visualizations - the differences between model types are less important than showing the overall correction capabilities

4. For any plots showing multiple lead times, focus your discussion on specific patterns across lead times (generally 1h, 6h, 12h, and 18h provide good sampling points)